{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-11 17:51:11 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:254: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(\n",
      "    \n",
      "[NeMo W 2024-10-11 17:51:11 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:265: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-10-11 17:51:11 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:325: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(\n",
      "    \n",
      "[NeMo W 2024-10-11 17:51:11 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:360: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "# from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAtaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriTTSDataset(Dataset):\n",
    "    def __init__(self, root_dir, sample_rate=22050, n_mels=80, n_fft=1024, hop_length=256, win_length=1024,target_duration = 5.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the root directory of the LibriTTS dataset.\n",
    "            sample_rate (int): Sampling rate for audio files.\n",
    "            n_mels (int): Number of mel bands.\n",
    "            n_fft (int): FFT size for mel spectrogram.\n",
    "            hop_length (int): Hop length for mel spectrogram.\n",
    "            win_length (int): Window length for mel spectrogram.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.target_duration = target_duration\n",
    "\n",
    "        # Initialize the MelSpectrogram transformation\n",
    "        self.mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_mels=n_mels,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            win_length=win_length,\n",
    "        )\n",
    "\n",
    "        # Collect all audio file paths and their corresponding labels (speaker IDs)\n",
    "        self.audio_files, self.labels, self.speaker_to_label = self._load_audio_paths_and_labels()\n",
    "\n",
    "    def _load_audio_paths_and_labels(self):\n",
    "        \"\"\"Load all audio file paths and assign a label to each speaker folder.\"\"\"\n",
    "        audio_files = []\n",
    "        labels = []\n",
    "        speaker_to_label = {}\n",
    "        speaker_id = 0  # Start labeling speakers from 0\n",
    "\n",
    "        # Assign a unique label to each speaker folder\n",
    "        for speaker_folder in os.listdir(self.root_dir):\n",
    "            speaker_path = os.path.join(self.root_dir, speaker_folder)\n",
    "            if os.path.isdir(speaker_path):\n",
    "                # Map speaker folder to a label\n",
    "                if speaker_folder not in speaker_to_label:\n",
    "                    speaker_to_label[speaker_folder] = speaker_id\n",
    "                    speaker_id += 1\n",
    "\n",
    "                # Traverse subfolders and find audio files\n",
    "                for subfolder in os.listdir(speaker_path):\n",
    "                    subfolder_path = os.path.join(speaker_path, subfolder)\n",
    "                    if os.path.isdir(subfolder_path):\n",
    "                        for file in os.listdir(subfolder_path):\n",
    "                            if file.endswith(\".wav\"):  # Assuming .wav audio format\n",
    "                                audio_files.append(os.path.join(subfolder_path, file))\n",
    "                                labels.append(speaker_to_label[speaker_folder])  # Assign the speaker's label\n",
    "\n",
    "        return audio_files, labels, speaker_to_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio\n",
    "        audio_path = self.audio_files[idx]\n",
    "        label = self.labels[idx]\n",
    "        # speaker_to_label = self.speaker_to_label[idx]\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "        # Resample audio if necessary\n",
    "        if sr != self.sample_rate:\n",
    "            resample_transform = T.Resample(orig_freq=sr, new_freq=self.sample_rate)\n",
    "            waveform = resample_transform(waveform)\n",
    "\n",
    "        target_length = int(self.target_duration * self.sample_rate)\n",
    "        audio_length = waveform.size(1)\n",
    "        \n",
    "        # Case 1: Trim the audio if it's too long\n",
    "        if audio_length > target_length:\n",
    "            waveform = waveform[:, :target_length]\n",
    "        \n",
    "        # Case 2: Pad the audio if it's too short\n",
    "        elif audio_length < target_length:\n",
    "            padding = target_length - audio_length\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "        \n",
    "        # return waveform\n",
    "\n",
    "        # Convert to mel spectrogram\n",
    "        mel_spectrogram = self.mel_transform(waveform)\n",
    "        mel_spectrogram = mel_spectrogram.squeeze()\n",
    "        # print(f\"speaker to label {mel_spectrogram.shape}\")\n",
    "\n",
    "        # Return the mel spectrogram and the corresponding speaker label\n",
    "        return mel_spectrogram, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_path = \"/home/keagan/Documents/projects/SelfVC/data/archive/\"\n",
    "# batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = LibriTTSDataset(train_data_path)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mel_spec = train_dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio.transforms as T\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate inputs and targets\n",
    "    inputs, targets = zip(*batch)\n",
    "    \n",
    "    # Pad sequences in the batch to the maximum length\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=-1)\n",
    "    \n",
    "    return inputs_padded, targets_padded\n",
    "\n",
    "# def pad_collate_fn(batch):\n",
    "#     \"\"\"\n",
    "#     Pads the Mel spectrograms in a batch to have the same time dimension.\n",
    "#     \"\"\"\n",
    "#     # Get the maximum length of the time dimension in the batch\n",
    "#     max_len = max([mel_spec[0].shape[2] for mel_spec in batch])\n",
    "\n",
    "#     # Pad each mel spectrogram to the max length in the batch\n",
    "#     padded_batch = []\n",
    "#     label_batch = []\n",
    "#     for mel_spec in batch:\n",
    "#         # Pad along the time dimension (dim=2)\n",
    "#         pad_amount = max_len - mel_spec[0].shape[2]\n",
    "#         padded_mel_spec = torch.nn.functional.pad(mel_spec[0], (0, pad_amount), mode='constant', value=0)\n",
    "#         padded_mel_spec = padded_mel_spec.squeeze()\n",
    "#         padded_batch.append(padded_mel_spec)\n",
    "#         label_batch.append(mel_spec[1])\n",
    "\n",
    "\n",
    "#     # Stack the batch into a tensor\n",
    "#     return torch.stack(padded_batch),torch.Tensor(label_batch)\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     mel_specs, labels = zip(*batch)\n",
    "#     print(mel_specs)\n",
    "    \n",
    "#     # Find the max time dimension for padding\n",
    "#     max_len = max([mel_specs.shape[2] for mel_spec in batch])\n",
    "    \n",
    "#     # Pad all Mel spectrograms to the same length\n",
    "#     mel_specs_padded = [torch.nn.functional.pad(mel, (0, max_len - mel.shape[2])) for mel in mel_specs]\n",
    "    \n",
    "#     mel_specs_padded = torch.stack(mel_specs_padded)  # (batch_size, n_mels, max_len)\n",
    "#     labels = torch.tensor(labels)  # (batch_size,)\n",
    "    \n",
    "#     return mel_specs_padded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libri_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)\n",
    "\n",
    "# # Test the DataLoader again\n",
    "# for i, mel_spectrogram_batch in enumerate(libri_dataloader):\n",
    "#     print(f\"Batch {i+1}\")\n",
    "#     # print(f\"Mel Spectrogram Batch Shape: {mel_spectrogram_batch[1]}\")\n",
    "#     print(mel_spectrogram_batch[1].shape)\n",
    "    \n",
    "#     # Break after a few batches to avoid printing everything\n",
    "#     if i == 2:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_pairs_with_labels(embeddings, speaker_labels):\n",
    "    \"\"\"\n",
    "    Generate positive and negative pairs for contrastive learning based on speaker labels.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (torch.Tensor): The feature embeddings of shape (batch_size, embedding_dim).\n",
    "        speaker_labels (torch.Tensor): Tensor of speaker labels of shape (batch_size).\n",
    "        \n",
    "    Returns:\n",
    "        tuple of (positive_pairs, negative_pairs): Indices for positive and negative pairs.\n",
    "    \"\"\"\n",
    "    positive_pairs = []\n",
    "    negative_pairs = []\n",
    "    \n",
    "    # Iterate through each possible pair in the batch\n",
    "    for i in range(len(speaker_labels)):\n",
    "        for j in range(i + 1, len(speaker_labels)):\n",
    "            if speaker_labels[i] == speaker_labels[j]:\n",
    "                # Positive pair (same speaker)\n",
    "                positive_pairs.append((i, j))\n",
    "            else:\n",
    "                # Negative pair (different speakers)\n",
    "                negative_pairs.append((i, j))\n",
    "    \n",
    "    return positive_pairs, negative_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_contrastive_loss(embeddings, positive_pairs, negative_pairs, margin=1.0):\n",
    "    \"\"\"\n",
    "    Compute contrastive loss given the embeddings, positive pairs, and negative pairs.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (torch.Tensor): The feature embeddings of shape (batch_size, embedding_dim).\n",
    "        positive_pairs (list): List of tuples containing indices of positive pairs.\n",
    "        negative_pairs (list): List of tuples containing indices of negative pairs.\n",
    "        margin (float): The margin for contrastive loss.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The computed contrastive loss.\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    for (i, j) in positive_pairs:\n",
    "        # Positive pairs: minimize the distance\n",
    "        distance = torch.nn.functional.pairwise_distance(embeddings[i], embeddings[j])\n",
    "        loss += distance.pow(2)  # Minimize positive distance (same speaker)\n",
    "\n",
    "    for (i, j) in negative_pairs:\n",
    "        # Negative pairs: maximize the distance up to the margin\n",
    "        distance = torch.nn.functional.pairwise_distance(embeddings[i], embeddings[j])\n",
    "        loss += torch.relu(margin - distance).pow(2)  # Maximize negative distance (different speakers)\n",
    "\n",
    "    return loss / (len(positive_pairs) + len(negative_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i, mel_spectrogram_batch in enumerate(libri_dataloader):\n",
    "#     print(f\"Batch {i+1}\")\n",
    "#     embeddings = torch.randn(64, 256)\n",
    "#     # print(f\"Mel Spectrogram Batch Shape: {mel_spectrogram_batch[1]}\")\n",
    "#     # print(mel_spectrogram_batch[1].shape)\n",
    "#     positive_pairs, negative_pairs = generate_pairs_with_labels(embeddings, mel_spectrogram_batch[1])\n",
    "#     print(len(positive_pairs),len(negative_pairs))\n",
    "#     loss = compute_contrastive_loss(embeddings, positive_pairs, negative_pairs, margin=1.0)\n",
    "#     print(loss)\n",
    "#     # Break after a few batches to avoid printing everything\n",
    "#     if i == 2:\n",
    "#         break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_masked_indices(sequence_length, mask_prob=0.15):\n",
    "#     \"\"\"\n",
    "#     Create masked indices for the MLM task.\n",
    "    \n",
    "#     Args:\n",
    "#         sequence_length (int): Length of the sequence to mask.\n",
    "#         mask_prob (float): Probability of masking a position.\n",
    "    \n",
    "#     Returns:\n",
    "#         torch.Tensor: A tensor of masked indices (1 for masked, 0 for unmasked).\n",
    "#     \"\"\"\n",
    "#     # Generate a mask of the same length as the sequence with 1 for masked positions\n",
    "#     mask = torch.bernoulli(torch.full((sequence_length,), mask_prob)).bool()\n",
    "#     return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_mask_to_embeddings(embeddings, mask_indices, mask_token=None):\n",
    "#     \"\"\"\n",
    "#     Apply masking to the embeddings based on the mask indices.\n",
    "    \n",
    "#     Args:\n",
    "#         embeddings (torch.Tensor): The input embeddings (batch_size, sequence_length, embedding_dim).\n",
    "#         mask_indices (torch.Tensor): Boolean mask indicating which positions to mask.\n",
    "#         mask_token (torch.Tensor): The token to replace masked positions (default: zero tensor).\n",
    "    \n",
    "#     Returns:\n",
    "#         torch.Tensor: Masked embeddings.\n",
    "#     \"\"\"\n",
    "#     if mask_token is None:\n",
    "#         # Default mask token is a tensor of zeros with the same dimension as each embedding\n",
    "#         mask_token = torch.zeros(embeddings.size(-1))\n",
    "\n",
    "#     # Clone the embeddings to avoid modifying the original\n",
    "#     masked_embeddings =                     \n",
    "#     # Apply mask token to the masked positions\n",
    "#     masked_embeddings[mask_indices] = mask_token\n",
    "    \n",
    "\n",
    "\n",
    "#     return masked_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_indices(sequence_length, mask_prob=0.15):\n",
    "    # Generate random mask indices for the sequence length\n",
    "    mask = torch.rand(sequence_length) < mask_prob\n",
    "    return mask\n",
    "\n",
    "def apply_mask_to_embeddings(mel_specs, mask_indices, mask_token_value=0.0):\n",
    "    \"\"\"\n",
    "    Apply a mask to the mel spectrogram embeddings across the sequence length for each sample in the batch.\n",
    "    \"\"\"\n",
    "    # mel_specs: [batch_size, num_mel_bands, sequence_length]\n",
    "    batch_size, num_mel_bands, sequence_length = mel_specs.shape\n",
    "\n",
    "    # Expand the mask for the batch size and mel bands\n",
    "    mask_indices_expanded = mask_indices.unsqueeze(0).unsqueeze(1).expand(batch_size, num_mel_bands, -1)\n",
    "    \n",
    "    # Clone the mel_specs and apply the mask token value\n",
    "    masked_mel_specs = mel_specs.clone()\n",
    "    masked_mel_specs[mask_indices_expanded] = mask_token_value\n",
    "    \n",
    "    return masked_mel_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_language_modeling_loss( masked_mel_specs,original_embeddings, model, mask_indices,lengths):\n",
    "    \"\"\"\n",
    "    Compute MLM loss for audio embeddings.\n",
    "    \n",
    "    Args:\n",
    "        original_embeddings (torch.Tensor): The original unmasked embeddings (batch_size, sequence_length, embedding_dim).\n",
    "        masked_embeddings (torch.Tensor): The masked embeddings (batch_size, sequence_length, embedding_dim).\n",
    "        model (nn.Module): The model to predict masked embeddings.\n",
    "        mask_indices (torch.Tensor): Boolean mask indicating which positions were masked.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The computed MLM loss.\n",
    "    \"\"\"\n",
    "    # Pass the masked embeddings through the model\n",
    "    # output_embeddings = model(masked_embeddings)\n",
    "    # lengths = torch.full((masked_mel_specs.shape[0],), masked_mel_specs.shape[2], dtype=torch.int32).cuda()\n",
    "    # output_embeddings, output_embeddings_length = model(\n",
    "    # audio_signal=masked_mel_specs,\n",
    "    # length=lengths\n",
    "    # )\n",
    "    model.eval()\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        output_embeddings, output_embeddings_length = model(\n",
    "            audio_signal=masked_mel_specs,\n",
    "            length=lengths\n",
    "        )\n",
    "    # Compute loss only on masked positions\n",
    "    # masked_original = original_embeddings[mask_indices]\n",
    "    # masked_output = output_embeddings[mask_indices]\n",
    "    \n",
    "    # Mean squared error for regression\n",
    "    mlm_loss = torch.nn.functional.mse_loss(output_embeddings, original_embeddings)\n",
    "    \n",
    "    return mlm_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerEncoder256(nn.Module):\n",
    "    def __init__(self, original_encoder):\n",
    "        super(ConformerEncoder256, self).__init__()\n",
    "        self.encoder = original_encoder\n",
    "        # Add a 1D convolution with stride to downsample to 256\n",
    "        self.downsample = nn.Conv1d(in_channels=512, out_channels=256, kernel_size=1, stride=2) # Example\n",
    "\n",
    "    def forward(self, audio_signal, length):\n",
    "        # Pass through original encoder\n",
    "        embeddings, lengths = self.encoder(audio_signal=audio_signal, length=length)\n",
    "        # Reduce the embedding size\n",
    "        embeddings = self.downsample(embeddings)\n",
    "        return embeddings, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-11 17:51:15 mixins:172] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-11 17:51:15 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /data/NeMo_ASR_SET/English/v2.0/train/tarred_audio_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20.0\n",
      "    min_duration: 0.1\n",
      "    shuffle_n: 2048\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths: /data/NeMo_ASR_SET/English/v2.0/train/audio__OP_0..4095_CL_.tar\n",
      "    \n",
      "[NeMo W 2024-10-11 17:51:15 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n",
      "[NeMo W 2024-10-11 17:51:15 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-11 17:51:15 features:289] PADDING: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-11 17:51:15 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/nemo/core/connectors/save_restore_connector.py:571: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "      return torch.load(model_weights, map_location='cpu')\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-11 17:51:16 save_restore_connector:249] Model EncDecCTCModelBPE was successfully restored from /home/keagan/.cache/huggingface/hub/models--nvidia--stt_en_conformer_ctc_large/snapshots/2c8326e4e43ae5b994612cfea3f3029818fb23c6/stt_en_conformer_ctc_large.nemo.\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\")\n",
    "asr_model = nemo_asr.models.EncDecCTCModel.from_pretrained(model_name=\"nvidia/stt_en_conformer_ctc_large\")\n",
    "conformer_encoder = asr_model.encoder.cuda()\n",
    "conformer_encoder_256 = ConformerEncoder256(conformer_encoder).cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 5e-5\n",
    "data_dir = '/home/keagan/Documents/projects/SelfVC/data/archive/'\n",
    "model_save_path = '/home/keagan/Documents/projects/SelfVC/models/conformer_encoder_v2.pth'\n",
    "log_file_path = '/home/keagan/Documents/projects/SelfVC/models/conformer_encoder_loss_v2.txt'\n",
    "optimizer = AdamW(conformer_encoder_256.parameters(), lr=learning_rate, betas=(0.9, 0.99))\n",
    "epochs = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LibriTTSDataset(data_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "# collate_fn=collate_fn\n",
    "# collate_fn=pad_collate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.06371981616147\n",
      "Epoch 2, Loss: 0.0495822630463261\n",
      "Epoch 3, Loss: 0.04609655121779176\n",
      "Epoch 4, Loss: 0.0455801553357741\n",
      "Epoch 5, Loss: 0.044317568078820105\n",
      "Epoch 6, Loss: 0.04859001682797153\n",
      "Epoch 7, Loss: 0.049594101679680314\n",
      "Epoch 8, Loss: 0.04172763413352461\n",
      "Epoch 9, Loss: 0.046485740978616914\n",
      "Epoch 10, Loss: 0.04475551787956634\n",
      "Epoch 11, Loss: 0.08151201486521786\n",
      "Epoch 12, Loss: 0.0914233225771839\n",
      "Epoch 13, Loss: 0.07487786081601816\n",
      "Epoch 14, Loss: 0.05147837367337169\n",
      "Epoch 15, Loss: 0.10193727297047309\n",
      "Epoch 16, Loss: 0.08307411835348708\n",
      "Epoch 17, Loss: 0.11479790850788649\n",
      "Epoch 18, Loss: 0.05346678588084645\n",
      "Epoch 19, Loss: 0.07168985201974683\n",
      "Epoch 20, Loss: 0.04347462046201157\n",
      "Epoch 21, Loss: 0.04009138548830088\n",
      "Epoch 22, Loss: 0.043061175456139196\n",
      "Epoch 23, Loss: 0.04249594732423614\n",
      "Epoch 24, Loss: 0.041954694554904554\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m conformer_encoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mel_specs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      5\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      6\u001b[0m     mel_specs \u001b[38;5;241m=\u001b[39m mel_specs\u001b[38;5;241m.\u001b[39mcuda()  \u001b[38;5;66;03m# (batch_size, n_mels, max_len)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[2], line 68\u001b[0m, in \u001b[0;36mLibriTTSDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sr \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_rate:\n\u001b[1;32m     67\u001b[0m     resample_transform \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mResample(orig_freq\u001b[38;5;241m=\u001b[39msr, new_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_rate)\n\u001b[0;32m---> 68\u001b[0m     waveform \u001b[38;5;241m=\u001b[39m \u001b[43mresample_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m target_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_duration \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_rate)\n\u001b[1;32m     71\u001b[0m audio_length \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:979\u001b[0m, in \u001b[0;36mResample.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_freq:\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m waveform\n\u001b[0;32m--> 979\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_sinc_resample_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torchaudio/functional/functional.py:1466\u001b[0m, in \u001b[0;36m_apply_sinc_resample_kernel\u001b[0;34m(waveform, orig_freq, new_freq, gcd, kernel, width)\u001b[0m\n\u001b[1;32m   1464\u001b[0m num_wavs, length \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1465\u001b[0m waveform \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(waveform, (width, width \u001b[38;5;241m+\u001b[39m orig_freq))\n\u001b[0;32m-> 1466\u001b[0m resampled \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morig_freq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m resampled \u001b[38;5;241m=\u001b[39m resampled\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(num_wavs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1468\u001b[0m target_length \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mceil(torch\u001b[38;5;241m.\u001b[39mas_tensor(new_freq \u001b[38;5;241m*\u001b[39m length \u001b[38;5;241m/\u001b[39m orig_freq))\u001b[38;5;241m.\u001b[39mlong()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):  # Train for 50 epochs\n",
    "    conformer_encoder.train()\n",
    "    total_loss = 0\n",
    "    for mel_specs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        mel_specs = mel_specs.cuda()  # (batch_size, n_mels, max_len)\n",
    "        labels = labels  # (batch_size,)\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        # embeddings = conformer_encoder(mel_specs)  # (batch_size, output_dim, max_len)\n",
    "        \n",
    "        # print(mel_specs.shape[2])\n",
    "        # print(torch.tensor(mel_specs.shape[2]))\n",
    "        lengths = torch.full((mel_specs.shape[0],), mel_specs.shape[2], dtype=torch.int32).cuda()\n",
    "        # print(lengths.shape)\n",
    "        # with autocast():\n",
    "        embeddings, embeddings_length = conformer_encoder_256(\n",
    "            audio_signal=mel_specs,\n",
    "            length=lengths\n",
    "        )\n",
    "        # print(\"Test\")\n",
    "        # Contrastive loss\n",
    "        positive_pairs, negative_pairs = generate_pairs_with_labels(embeddings, labels)\n",
    "        contrastive_loss = compute_contrastive_loss(embeddings, positive_pairs, negative_pairs)\n",
    "        contrastive_loss = contrastive_loss.mean()\n",
    "        # MLM loss (masked positions)\n",
    "        mask_indices = create_masked_indices(sequence_length=mel_specs.shape[2])\n",
    "        masked_mel_specs = apply_mask_to_embeddings(mel_specs, mask_indices)\n",
    "        # masked_mel_specs = apply_mask_to_embeddings(mel_specs, mask_indices)\n",
    "        mlm_loss = masked_language_modeling_loss(masked_mel_specs, embeddings,conformer_encoder_256, mask_indices,lengths)\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = contrastive_loss + mlm_loss\n",
    "        total_loss += loss.item()\n",
    "        # print(contrastive_loss,mlm_loss)\n",
    "        with open(log_file_path, 'a') as log_file:\n",
    "            log_file.write(f'Contrastive Loss: {contrastive_loss:.6f} MLM Loss: {mlm_loss:.6f}\\n')\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # scaler.scale(loss).backward()\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\\n')    \n",
    "    torch.save(conformer_encoder_256, model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
