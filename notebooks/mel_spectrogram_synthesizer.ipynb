{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torch.nn as nn\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from typing import Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-17 09:13:35 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/nemo/collections/tts/modules/common.py:206: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "      @amp.autocast(False)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-17 09:13:36 cloud:58] Found existing object /home/keagan/.cache/torch/NeMo/NeMo_1.23.0/tts_en_fastpitch_align/b7d086a07b5126c12d5077d9a641a38c/tts_en_fastpitch_align.nemo.\n",
      "[NeMo I 2024-10-17 09:13:36 cloud:64] Re-using file from: /home/keagan/.cache/torch/NeMo/NeMo_1.23.0/tts_en_fastpitch_align/b7d086a07b5126c12d5077d9a641a38c/tts_en_fastpitch_align.nemo\n",
      "[NeMo I 2024-10-17 09:13:36 common:924] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " NeMo-text-processing :: INFO     :: Creating ClassifyFst grammars.\n",
      "[NeMo W 2024-10-17 09:13:48 en_us_arpabet:66] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.\n",
      "[NeMo W 2024-10-17 09:13:48 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
      "      manifest_filepath: /ws/LJSpeech/nvidia_ljspeech_train_clean_ngc.json\n",
      "      sample_rate: 22050\n",
      "      sup_data_path: /raid/LJSpeech/supplementary\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      n_fft: 1024\n",
      "      win_length: 1024\n",
      "      hop_length: 256\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: null\n",
      "      min_duration: 0.1\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65.40639132514966\n",
      "      pitch_fmax: 2093.004522404789\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 212.35873413085938\n",
      "      pitch_std: 68.52806091308594\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 24\n",
      "      num_workers: 0\n",
      "    \n",
      "[NeMo W 2024-10-17 09:13:48 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
      "      manifest_filepath: /ws/LJSpeech/nvidia_ljspeech_val_clean_ngc.json\n",
      "      sample_rate: 22050\n",
      "      sup_data_path: /raid/LJSpeech/supplementary\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      n_fft: 1024\n",
      "      win_length: 1024\n",
      "      hop_length: 256\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: null\n",
      "      min_duration: null\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65.40639132514966\n",
      "      pitch_fmax: 2093.004522404789\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 212.35873413085938\n",
      "      pitch_std: 68.52806091308594\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 24\n",
      "      num_workers: 0\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-17 09:13:48 features:289] PADDING: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-17 09:13:48 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/nemo/core/connectors/save_restore_connector.py:571: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "      return torch.load(model_weights, map_location='cpu')\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-17 09:13:49 save_restore_connector:249] Model FastPitchModel was successfully restored from /home/keagan/.cache/torch/NeMo/NeMo_1.23.0/tts_en_fastpitch_align/b7d086a07b5126c12d5077d9a641a38c/tts_en_fastpitch_align.nemo.\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.tts.models import FastPitchModel\n",
    "spec_generator = FastPitchModel.from_pretrained(\"tts_en_fastpitch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 10:54:09 fastpitch:291] parse() is meant to be called in eval mode.\n",
      " NeMo-text-processing :: INFO     :: Skipping post-processing of You can type your sentence here to get nemo to produce speech dot This is a test to see how well fast pitch is for '.'\n",
      "[NeMo W 2024-10-15 10:54:09 fastpitch:368] generate_spectrogram() is meant to be called in eval mode.\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "parsed = spec_generator.parse(\"You can type your sentence here to get nemo to produce speech.This is a test to see how well fast pitch is\")\n",
    "spectrogram = spec_generator.generate_spectrogram(tokens=parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 642])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectrogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fe_encoder = spec_generator.fastpitch.encoder\n",
    "Fd_decoder = spec_generator.fastpitch.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FFTransformerEncoder(\n",
       "  (pos_emb): PositionalEmbedding()\n",
       "  (drop): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerLayer(\n",
       "      (dec_attn): MultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.1, inplace=False)\n",
       "        (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "        (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_ff): PositionwiseConvFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): ReLU()\n",
       "          (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cond_input): ConditionalInput()\n",
       "  (word_emb): Embedding(115, 384, padding_idx=112)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fe_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerEncoder256(nn.Module):\n",
    "    def __init__(self, original_encoder):\n",
    "        super(ConformerEncoder256, self).__init__()\n",
    "        self.encoder = original_encoder\n",
    "        # Add a 1D convolution with stride to downsample to 256\n",
    "        self.downsample = nn.Conv1d(in_channels=512, out_channels=256, kernel_size=1, stride=2) # Example\n",
    "\n",
    "    def forward(self, audio_signal, length):\n",
    "        # Pass through original encoder\n",
    "        embeddings, lengths = self.encoder(audio_signal=audio_signal, length=length)\n",
    "        # Reduce the embedding size\n",
    "        embeddings = self.downsample(embeddings)\n",
    "        return embeddings, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    return F.cosine_similarity(v1.unsqueeze(0), v2.unsqueeze(0), dim=-1).item()\n",
    "\n",
    "def group_similar_vectors(vectors, threshold, vector_duration):\n",
    "    grouped_vectors = []\n",
    "    durations = []\n",
    "\n",
    "    current_group = [vectors[:, 0]]  # Start with the first vector (slice across batch dimension)\n",
    "    current_duration = vector_duration\n",
    "\n",
    "    for i in range(1, vectors.shape[-1]):\n",
    "        sim = cosine_similarity(vectors[:, i], vectors[:, i - 1])\n",
    "        # print(sim)\n",
    "        # If cosine similarity is above threshold, group the vectors\n",
    "        if sim > threshold:\n",
    "            # print(\"Exceeded Threshold\")\n",
    "            current_group.append(vectors[:, i])\n",
    "            current_duration += vector_duration\n",
    "        else:\n",
    "            # Compute the average of the current group and save it\n",
    "            averaged_vector = torch.mean(torch.stack(current_group, dim=-1), dim=-1)\n",
    "            grouped_vectors.append(averaged_vector)\n",
    "            durations.append(current_duration)\n",
    "\n",
    "            # Start a new group\n",
    "            current_group = [vectors[:, i]]\n",
    "            current_duration = vector_duration\n",
    "\n",
    "    # Append the last group\n",
    "    if current_group:\n",
    "        averaged_vector = torch.mean(torch.stack(current_group, dim=-1), dim=-1)\n",
    "        grouped_vectors.append(averaged_vector)\n",
    "        durations.append(current_duration)\n",
    "\n",
    "    return torch.stack(grouped_vectors, dim=-1), durations\n",
    "\n",
    "# Example usage\n",
    "# Assume `content_embeddings` is the input tensor of shape [1, 256, num_content_vectors]\n",
    "# `T`: Cosine similarity threshold\n",
    "# `vector_duration`: The duration of a single vector (calculated as ~46.44 milliseconds)\n",
    "\n",
    "def duration_augmented_representation(content_embeddings, T=0.9, vector_duration=46.44):\n",
    "    # Remove the batch dimension for processing\n",
    "    content_vectors = content_embeddings.squeeze(0)  # Shape: [256, num_content_vectors]\n",
    "\n",
    "    grouped_vectors, new_durations = group_similar_vectors(content_vectors, T, vector_duration)\n",
    "\n",
    "    # Convert durations to seconds if needed\n",
    "    new_durations_in_seconds = [d / 1000 for d in new_durations]\n",
    "\n",
    "    # Add the batch dimension back to the output\n",
    "    grouped_vectors = grouped_vectors.unsqueeze(0)  # Shape: [1, 256, num_grouped_vectors]\n",
    "\n",
    "    return grouped_vectors, new_durations_in_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 10:54:40 nemo_logging:349] /tmp/ipykernel_103650/1129351816.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "      conformer_encoder_256 = torch.load('/home/keagan/Documents/projects/SelfVC/models/conformer_encoder_v2.pth')\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "audio_path = '/home/keagan/Documents/projects/SelfVC/data/audios/14_208_000001_000000.wav'\n",
    "conformer_encoder_256 = torch.load('/home/keagan/Documents/projects/SelfVC/models/conformer_encoder_v2.pth')\n",
    "mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=22050,\n",
    "            n_mels=80,\n",
    "            n_fft=1024,\n",
    "            hop_length=256,\n",
    "            win_length=1024,\n",
    "        )\n",
    "waveform, sr = torchaudio.load(audio_path)\n",
    "mel_spectrogram = mel_transform(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 846])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mel_spectrogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_vectors = torch.randn(1,256,100)  # Replace with actual model output\n",
    "\n",
    "T = 0.925  # Cosine similarity threshold\n",
    "grouped_vectors, new_durations = duration_augmented_representation(mel_spectrogram, T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 350])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = torch.full((mel_spectrogram.shape[0],), mel_spectrogram.shape[2], dtype=torch.int32)\n",
    "# print(lengths.shape)\n",
    "# with autocast():\n",
    "embeddings, embeddings_length = conformer_encoder_256(\n",
    "    audio_signal=mel_spectrogram.cuda(),\n",
    "    length=lengths.cuda()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 106])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_vectors, new_durations = duration_augmented_representation(embeddings, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.922639999999997]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Example input: token IDs (Batch, Sequence Length)(Phoneme SEquence)\n",
    "input_tokens = torch.randint(0, 100, (1,256)).cuda()\n",
    "\n",
    "# Pass through FastPitch encoder\n",
    "encoder_outputs = Fe_encoder(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 384])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m x_pooled \u001b[38;5;241m=\u001b[39m global_avg_pooling(output_vectors)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# input_tensor = x_pooled.to(torch.int)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mFe_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_pooled\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/nemo/collections/tts/modules/transformer.py:304\u001b[0m, in \u001b[0;36mFFTransformerEncoder.forward\u001b[0;34m(self, input, conditioning)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, conditioning\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m, (\u001b[38;5;28minput\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m), conditioning)\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# Fe_encoder = Fe_encoder.cuda()\n",
    "import torch.nn as nn\n",
    "\n",
    "global_avg_pooling = nn.AdaptiveAvgPool1d(384)  # Set the output size to 384\n",
    "\n",
    "output_vectors = torch.randn(1,256,200)\n",
    "# .cuda()  # Replace with actual model output\n",
    "x_pooled = global_avg_pooling(output_vectors)\n",
    "# input_tensor = x_pooled.to(torch.int)\n",
    "Fe_encoder(input=x_pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/IPython/core/formatters.py:711\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    704\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    705\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    707\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    708\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    709\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    710\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 711\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/IPython/lib/pretty.py:419\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    408\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    409\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    410\u001b[0m                     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m    411\u001b[0m                     \u001b[38;5;66;03m# check if cls defines __repr__\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(_safe_getattr(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    418\u001b[0m                 ):\n\u001b[0;32m--> 419\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/IPython/lib/pretty.py:787\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/_tensor.py:463\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    460\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    461\u001b[0m     )\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/_tensor_str.py:698\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    697\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/_tensor_str.py:618\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    616\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    617\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 618\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    621\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/_tensor_str.py:350\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    348\u001b[0m     )\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m _Formatter(\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m summarize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/_tensor_str.py:388\u001b[0m, in \u001b[0;36mget_summarized_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([get_summarized_data(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (start \u001b[38;5;241m+\u001b[39m end)])\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([get_summarized_data(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/_tensor_str.py:388\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([get_summarized_data(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (start \u001b[38;5;241m+\u001b[39m end)])\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/_tensor_str.py:386\u001b[0m, in \u001b[0;36mget_summarized_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m     start \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems)]\n\u001b[1;32m    385\u001b[0m     end \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))]\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([get_summarized_data(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (start \u001b[38;5;241m+\u001b[39m end)])\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([get_summarized_data(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/_tensor_str.py:386\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    384\u001b[0m     start \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems)]\n\u001b[1;32m    385\u001b[0m     end \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))]\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (start \u001b[38;5;241m+\u001b[39m end)])\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([get_summarized_data(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/_tensor_str.py:376\u001b[0m, in \u001b[0;36mget_summarized_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems:\n\u001b[0;32m--> 376\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mPRINT_OPTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medgeitems\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mPRINT_OPTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medgeitems\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "output_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FFTransformerEncoder(\n",
       "  (pos_emb): PositionalEmbedding()\n",
       "  (drop): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerLayer(\n",
       "      (dec_attn): MultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.1, inplace=False)\n",
       "        (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "        (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_ff): PositionwiseConvFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): ReLU()\n",
       "          (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cond_input): ConditionalInput()\n",
       "  (word_emb): Embedding(115, 384, padding_idx=112)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Fe_encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Fe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseConvFF(nn.Module):\n",
    "    def __init__(self, d_model, d_inner, kernel_size, dropout, pre_lnorm=False):\n",
    "        super(PositionwiseConvFF, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.CoreNet = nn.Sequential(\n",
    "            nn.Conv1d(d_model, d_inner, kernel_size, 1, (kernel_size // 2)),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout),  # worse convergence\n",
    "            nn.Conv1d(d_inner, d_model, kernel_size, 1, (kernel_size // 2)),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.pre_lnorm = pre_lnorm\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return self._forward(inp)\n",
    "\n",
    "    def _forward(self, inp):\n",
    "        if self.pre_lnorm:\n",
    "            # layer normalization + positionwise feed-forward\n",
    "            core_out = inp.transpose(1, 2)\n",
    "            core_out = self.CoreNet(self.layer_norm(core_out).to(inp.dtype))\n",
    "            core_out = core_out.transpose(1, 2)\n",
    "\n",
    "            # residual connection\n",
    "            output = core_out + inp\n",
    "        else:\n",
    "            # positionwise feed-forward\n",
    "            core_out = inp.transpose(1, 2)\n",
    "            core_out = self.CoreNet(core_out)\n",
    "            core_out = core_out.transpose(1, 2)\n",
    "\n",
    "            # residual connection + layer normalization\n",
    "            output = self.layer_norm(inp + core_out).to(inp.dtype)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttn(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0.1,\n",
    "                 pre_lnorm=False):\n",
    "        super(MultiHeadAttn, self).__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_head\n",
    "        self.scale = 1 / (d_head ** 0.5)\n",
    "        self.pre_lnorm = pre_lnorm\n",
    "\n",
    "        self.qkv_net = nn.Linear(d_model, 3 * n_head * d_head)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.dropatt = nn.Dropout(dropatt)\n",
    "        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, inp, attn_mask=None):\n",
    "        return self._forward(inp, attn_mask)\n",
    "\n",
    "    def _forward(self, inp, attn_mask=None):\n",
    "        residual = inp\n",
    "\n",
    "        if self.pre_lnorm:\n",
    "            # layer normalization\n",
    "            inp = self.layer_norm(inp)\n",
    "\n",
    "        n_head, d_head = self.n_head, self.d_head\n",
    "\n",
    "        head_q, head_k, head_v = torch.chunk(self.qkv_net(inp), 3, dim=2)\n",
    "        head_q = head_q.view(inp.size(0), inp.size(1), n_head, d_head)\n",
    "        head_k = head_k.view(inp.size(0), inp.size(1), n_head, d_head)\n",
    "        head_v = head_v.view(inp.size(0), inp.size(1), n_head, d_head)\n",
    "\n",
    "        q = head_q.permute(2, 0, 1, 3).reshape(-1, inp.size(1), d_head)\n",
    "        k = head_k.permute(2, 0, 1, 3).reshape(-1, inp.size(1), d_head)\n",
    "        v = head_v.permute(2, 0, 1, 3).reshape(-1, inp.size(1), d_head)\n",
    "\n",
    "        attn_score = torch.bmm(q, k.transpose(1, 2))\n",
    "        attn_score.mul_(self.scale)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask.unsqueeze(1).to(attn_score.dtype)\n",
    "            attn_mask = attn_mask.repeat(n_head, attn_mask.size(2), 1)\n",
    "            attn_score.masked_fill_(attn_mask.to(torch.bool), -float('inf'))\n",
    "\n",
    "        attn_prob = F.softmax(attn_score, dim=2)\n",
    "        attn_prob = self.dropatt(attn_prob)\n",
    "        attn_vec = torch.bmm(attn_prob, v)\n",
    "\n",
    "        attn_vec = attn_vec.view(n_head, inp.size(0), inp.size(1), d_head)\n",
    "        attn_vec = attn_vec.permute(1, 2, 0, 3).contiguous().view(\n",
    "            inp.size(0), inp.size(1), n_head * d_head)\n",
    "\n",
    "        # linear projection\n",
    "        attn_out = self.o_net(attn_vec)\n",
    "        attn_out = self.drop(attn_out)\n",
    "\n",
    "        if self.pre_lnorm:\n",
    "            # residual connection\n",
    "            output = residual + attn_out\n",
    "        else:\n",
    "            # residual connection + layer normalization\n",
    "            output = self.layer_norm(residual + attn_out)\n",
    "\n",
    "        output = output.to(attn_out.dtype)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_head, d_inner, kernel_size, dropout,\n",
    "                 **kwargs):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        self.dec_attn = MultiHeadAttn(n_head, d_model, d_head, dropout, **kwargs)\n",
    "        self.pos_ff = PositionwiseConvFF(d_model, d_inner, kernel_size, dropout,\n",
    "                                         pre_lnorm=kwargs.get('pre_lnorm'))\n",
    "\n",
    "    def forward(self, dec_inp, mask=None):\n",
    "        # output = self.dec_attn(dec_inp, attn_mask=~mask.squeeze(2))\n",
    "        output = self.dec_attn(dec_inp)\n",
    "        # output *= mask\n",
    "        output = self.pos_ff(output)\n",
    "        # output *= mask\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_from_lens(lens, max_len: Optional[int] = None):\n",
    "    if max_len is None:\n",
    "        max_len = lens.max()\n",
    "    ids = torch.arange(0, max_len, device=lens.device, dtype=lens.dtype)\n",
    "    mask = torch.lt(ids, lens.unsqueeze(1))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTransformer(nn.Module):\n",
    "    def __init__(self, n_layer, n_head, d_model, d_head, d_inner, kernel_size,\n",
    "                 dropout, dropatt, dropemb=0.0, embed_input=True,\n",
    "                 n_embed=None, d_embed=None, padding_idx=0, pre_lnorm=False):\n",
    "        super(FFTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_head\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        # if embed_input:\n",
    "        #     self.word_emb = nn.Embedding(n_embed, d_embed or d_model,\n",
    "        #                                  padding_idx=self.padding_idx)\n",
    "        # else:\n",
    "        #     self.word_emb = None\n",
    "\n",
    "        # self.pos_emb = PositionalEmbedding(self.d_model)\n",
    "        self.drop = nn.Dropout(dropemb)\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for _ in range(n_layer):\n",
    "            self.layers.append(\n",
    "                TransformerLayer(\n",
    "                    n_head, d_model, d_head, d_inner, kernel_size, dropout,\n",
    "                    dropatt=dropatt, pre_lnorm=pre_lnorm)\n",
    "            )\n",
    "\n",
    "    def forward(self, dec_inp, seq_lens=None, conditioning=0):\n",
    "        # if self.word_emb is None:\n",
    "        #     inp = dec_inp\n",
    "        #     mask = mask_from_lens(seq_lens).unsqueeze(2)\n",
    "        # else:\n",
    "        #     inp = self.word_emb(dec_inp)\n",
    "        #     # [bsz x L x 1]\n",
    "        #     mask = (dec_inp != self.padding_idx).unsqueeze(2)\n",
    "\n",
    "        # pos_seq = torch.arange(inp.size(1), device=inp.device).to(inp.dtype)\n",
    "        # pos_emb = self.pos_emb(pos_seq) * mask\n",
    "\n",
    "        # out = self.drop(inp + pos_emb + conditioning)\n",
    "        # mask = mask_from_lens(seq_lens).unsqueeze(2)\n",
    "        # mask = (dec_inp != self.padding_idx).unsqueeze(2)\n",
    "        # print(mask.shape)\n",
    "        out = self.drop(dec_inp + conditioning)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # out = layer(out, mask=mask)\n",
    "            out = layer(out)\n",
    "\n",
    "        # out = self.drop(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_encoder = FFTransformer(\n",
    "    n_layer=6, n_head=1,\n",
    "    d_model=512,\n",
    "    d_head=64,\n",
    "    d_inner=1536,\n",
    "    kernel_size=3,\n",
    "    dropout=0.1,\n",
    "    dropatt=0.1,\n",
    "    dropemb=0.0,\n",
    "    embed_input=True,\n",
    "    d_embed=384,\n",
    "    n_embed=None,\n",
    "    padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FFTransformer(\n",
       "  (drop): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerLayer(\n",
       "      (dec_attn): MultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=512, out_features=192, bias=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.1, inplace=False)\n",
       "        (o_net): Linear(in_features=64, out_features=512, bias=False)\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_ff): PositionwiseConvFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Conv1d(512, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): ReLU()\n",
       "          (2): Conv1d(1536, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "global_avg_pooling = nn.AdaptiveAvgPool1d(512)  # Set the output size to 384\n",
    "\n",
    "output_vectors = torch.randn(1,256,200)\n",
    "# .cuda()  # Replace with actual model output\n",
    "x_pooled = global_avg_pooling(output_vectors)\n",
    "# input_tensor = x_pooled.to(torch.int)\n",
    "# Fe_encoder(input=x_pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 512])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vectors = torch.randn(1,256,512) #(batch,timesteps,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enc_out = custom_encoder(output_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 512])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dec_out = custom_encoder(test_enc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 512])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dec_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PitchPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_predictor = spec_generator.fastpitch.pitch_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TemporalPredictor(\n",
       "  (cond_input): ConditionalInput()\n",
       "  (layers): ModuleList(\n",
       "    (0): ConvReLUNorm(\n",
       "      (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (norm): ConditionalLayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): ConvReLUNorm(\n",
       "      (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (norm): ConditionalLayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitch_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom PitchPredictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvReLUNorm(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, dropout=0.0):\n",
    "        super(ConvReLUNorm, self).__init__()\n",
    "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
    "                                    kernel_size=kernel_size,\n",
    "                                    padding=(kernel_size // 2))\n",
    "        self.norm = torch.nn.LayerNorm(out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, signal):\n",
    "        out = F.relu(self.conv(signal))\n",
    "        out = self.norm(out.transpose(1, 2)).transpose(1, 2).to(signal.dtype)\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalPredictor(nn.Module):\n",
    "    \"\"\"Predicts a single float per each temporal location\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, filter_size, kernel_size, dropout,\n",
    "                 n_layers=2, n_predictions=1):\n",
    "        super(TemporalPredictor, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(*[\n",
    "            ConvReLUNorm(input_size if i == 0 else filter_size, filter_size,\n",
    "                         kernel_size=kernel_size, dropout=dropout)\n",
    "            for i in range(n_layers)]\n",
    "        )\n",
    "        self.n_predictions = n_predictions\n",
    "        self.fc = nn.Linear(filter_size, self.n_predictions, bias=True)\n",
    "\n",
    "    def forward(self, enc_out):\n",
    "        # enc_out_mask\n",
    "        out = enc_out \n",
    "        # * enc_out_mask\n",
    "        out = self.layers(out.transpose(1, 2)).transpose(1, 2)\n",
    "        out = self.fc(out) \n",
    "        # * enc_out_mask\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_predictor = TemporalPredictor(\n",
    "        512,\n",
    "        filter_size=256,\n",
    "        kernel_size=3,\n",
    "        dropout=0.1, n_layers=2,\n",
    "        n_predictions=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TemporalPredictor(\n",
       "  (layers): Sequential(\n",
       "    (0): ConvReLUNorm(\n",
       "      (conv): Conv1d(512, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): ConvReLUNorm(\n",
       "      (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitch_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 512])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vectors = torch.randn(1,100,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_pred_out = pitch_predictor(output_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 1])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitch_pred_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pitch_contour = np.random.rand(50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitch_contour.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duration Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_predictor = spec_generator.fastpitch.duration_predictor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
