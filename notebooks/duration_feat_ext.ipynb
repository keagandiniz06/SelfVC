{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-17 10:37:40 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:254: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(\n",
      "    \n",
      "[NeMo W 2024-10-17 10:37:40 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:265: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-10-17 10:37:40 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:325: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(\n",
      "    \n",
      "[NeMo W 2024-10-17 10:37:40 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:360: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torch.nn as nn\n",
    "import nemo.collections.asr as nemo_asr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duration Augmented Content Vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    return F.cosine_similarity(v1.unsqueeze(0), v2.unsqueeze(0), dim=-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_similar_vectors(vectors, threshold, vector_duration):\n",
    "    grouped_vectors = []\n",
    "    durations = []\n",
    "\n",
    "    current_group = [vectors[:, 0]]  # Start with the first vector (slice across batch dimension)\n",
    "    current_duration = vector_duration\n",
    "\n",
    "    for i in range(1, vectors.shape[-1]):\n",
    "        sim = cosine_similarity(vectors[:, i], vectors[:, i - 1])\n",
    "        # print(sim)\n",
    "        # If cosine similarity is above threshold, group the vectors\n",
    "        if sim > threshold:\n",
    "            # print(\"Exceeded Threshold\")\n",
    "            current_group.append(vectors[:, i])\n",
    "            current_duration += vector_duration\n",
    "        else:\n",
    "            # Compute the average of the current group and save it\n",
    "            averaged_vector = torch.mean(torch.stack(current_group, dim=-1), dim=-1)\n",
    "            grouped_vectors.append(averaged_vector)\n",
    "            durations.append(current_duration)\n",
    "\n",
    "            # Start a new group\n",
    "            current_group = [vectors[:, i]]\n",
    "            current_duration = vector_duration\n",
    "\n",
    "    # Append the last group\n",
    "    if current_group:\n",
    "        averaged_vector = torch.mean(torch.stack(current_group, dim=-1), dim=-1)\n",
    "        grouped_vectors.append(averaged_vector)\n",
    "        durations.append(current_duration)\n",
    "\n",
    "    return torch.stack(grouped_vectors, dim=-1), durations\n",
    "\n",
    "# Example usage\n",
    "# Assume `content_embeddings` is the input tensor of shape [1, 256, num_content_vectors]\n",
    "# `T`: Cosine similarity threshold\n",
    "# `vector_duration`: The duration of a single vector (calculated as ~46.44 milliseconds)\n",
    "\n",
    "def duration_augmented_representation(content_embeddings, T=0.925, vector_duration=46.44):\n",
    "    # Remove the batch dimension for processing\n",
    "    content_vectors = content_embeddings.squeeze(0)  # Shape: [256, num_content_vectors]\n",
    "\n",
    "    grouped_vectors, new_durations = group_similar_vectors(content_vectors, T, vector_duration)\n",
    "\n",
    "    # Convert durations to seconds if needed\n",
    "    new_durations_in_seconds = [d / 1000 for d in new_durations]\n",
    "\n",
    "    # Add the batch dimension back to the output\n",
    "    grouped_vectors = grouped_vectors.unsqueeze(0)  # Shape: [1, 256, num_grouped_vectors]\n",
    "\n",
    "    return grouped_vectors, new_durations_in_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerEncoder256(nn.Module):\n",
    "    def __init__(self, original_encoder):\n",
    "        super(ConformerEncoder256, self).__init__()\n",
    "        self.encoder = original_encoder\n",
    "        # Add a 1D convolution with stride to downsample to 256\n",
    "        self.downsample = nn.Conv1d(in_channels=512, out_channels=256, kernel_size=1, stride=2) # Example\n",
    "\n",
    "    def forward(self, audio_signal, length):\n",
    "        # Pass through original encoder\n",
    "        embeddings, lengths = self.encoder(audio_signal=audio_signal, length=length)\n",
    "        # Reduce the embedding size\n",
    "        embeddings = self.downsample(embeddings)\n",
    "        return embeddings, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-17 10:38:10 nemo_logging:349] /tmp/ipykernel_350821/1129351816.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "      conformer_encoder_256 = torch.load('/home/keagan/Documents/projects/SelfVC/models/conformer_encoder_v2.pth')\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "audio_path = '/home/keagan/Documents/projects/SelfVC/data/audios/14_208_000001_000000.wav'\n",
    "conformer_encoder_256 = torch.load('/home/keagan/Documents/projects/SelfVC/models/conformer_encoder_v2.pth')\n",
    "mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=22050,\n",
    "            n_mels=80,\n",
    "            n_fft=1024,\n",
    "            hop_length=256,\n",
    "            win_length=1024,\n",
    "        )\n",
    "waveform, sr = torchaudio.load(audio_path)\n",
    "mel_spectrogram = mel_transform(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_vectors = torch.randn(1,256,100)  # Replace with actual model output\n",
    "\n",
    "T = 0.9  # Cosine similarity threshold\n",
    "lengths = torch.full((mel_spectrogram.shape[0],), mel_spectrogram.shape[2], dtype=torch.int32)\n",
    "# print(lengths.shape)\n",
    "# with autocast():\n",
    "embeddings, embeddings_length = conformer_encoder_256(\n",
    "    audio_signal=mel_spectrogram.cuda(),\n",
    "    length=lengths.cuda()\n",
    ")\n",
    "grouped_vectors, new_durations = duration_augmented_representation(embeddings, T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.922639999999997]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
