{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-11 17:14:47 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:254: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(\n",
      "    \n",
      "[NeMo W 2024-10-11 17:14:47 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:265: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-10-11 17:14:47 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:325: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(\n",
      "    \n",
      "[NeMo W 2024-10-11 17:14:47 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:360: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from torch.optim import AdamW\n",
    "# from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAtaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriTTSDataset(Dataset):\n",
    "    def __init__(self, root_dir, sample_rate=22050, n_mels=80, n_fft=1024, hop_length=256, win_length=1024,target_duration = 5.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the root directory of the LibriTTS dataset.\n",
    "            sample_rate (int): Sampling rate for audio files.\n",
    "            n_mels (int): Number of mel bands.\n",
    "            n_fft (int): FFT size for mel spectrogram.\n",
    "            hop_length (int): Hop length for mel spectrogram.\n",
    "            win_length (int): Window length for mel spectrogram.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.target_duration = target_duration\n",
    "\n",
    "        # Initialize the MelSpectrogram transformation\n",
    "        self.mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_mels=n_mels,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            win_length=win_length,\n",
    "        )\n",
    "\n",
    "        # Collect all audio file paths and their corresponding labels (speaker IDs)\n",
    "        self.audio_files, self.labels, self.speaker_to_label = self._load_audio_paths_and_labels()\n",
    "\n",
    "    def _load_audio_paths_and_labels(self):\n",
    "        \"\"\"Load all audio file paths and assign a label to each speaker folder.\"\"\"\n",
    "        audio_files = []\n",
    "        labels = []\n",
    "        speaker_to_label = {}\n",
    "        speaker_id = 0  # Start labeling speakers from 0\n",
    "\n",
    "        # Assign a unique label to each speaker folder\n",
    "        for speaker_folder in os.listdir(self.root_dir):\n",
    "            speaker_path = os.path.join(self.root_dir, speaker_folder)\n",
    "            if os.path.isdir(speaker_path):\n",
    "                # Map speaker folder to a label\n",
    "                if speaker_folder not in speaker_to_label:\n",
    "                    speaker_to_label[speaker_folder] = speaker_id\n",
    "                    speaker_id += 1\n",
    "\n",
    "                # Traverse subfolders and find audio files\n",
    "                for subfolder in os.listdir(speaker_path):\n",
    "                    subfolder_path = os.path.join(speaker_path, subfolder)\n",
    "                    if os.path.isdir(subfolder_path):\n",
    "                        for file in os.listdir(subfolder_path):\n",
    "                            if file.endswith(\".wav\"):  # Assuming .wav audio format\n",
    "                                audio_files.append(os.path.join(subfolder_path, file))\n",
    "                                labels.append(speaker_to_label[speaker_folder])  # Assign the speaker's label\n",
    "\n",
    "        return audio_files, labels, speaker_to_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio\n",
    "        audio_path = self.audio_files[idx]\n",
    "        label = self.labels[idx]\n",
    "        # speaker_to_label = self.speaker_to_label[idx]\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "        # Resample audio if necessary\n",
    "        if sr != self.sample_rate:\n",
    "            resample_transform = T.Resample(orig_freq=sr, new_freq=self.sample_rate)\n",
    "            waveform = resample_transform(waveform)\n",
    "\n",
    "        target_length = int(self.target_duration * self.sample_rate)\n",
    "        audio_length = waveform.size(1)\n",
    "        \n",
    "        # Case 1: Trim the audio if it's too long\n",
    "        if audio_length > target_length:\n",
    "            waveform = waveform[:, :target_length]\n",
    "        \n",
    "        # Case 2: Pad the audio if it's too short\n",
    "        elif audio_length < target_length:\n",
    "            padding = target_length - audio_length\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "        \n",
    "        # return waveform\n",
    "\n",
    "        # Convert to mel spectrogram\n",
    "        mel_spectrogram = self.mel_transform(waveform)\n",
    "        mel_spectrogram = mel_spectrogram.squeeze()\n",
    "        # print(f\"speaker to label {mel_spectrogram.shape}\")\n",
    "\n",
    "        # Return the mel spectrogram and the corresponding speaker label\n",
    "        return mel_spectrogram, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_path = \"/home/keagan/Documents/projects/SelfVC/data/archive/\"\n",
    "# batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = LibriTTSDataset(train_data_path)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mel_spec = train_dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio.transforms as T\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate inputs and targets\n",
    "    inputs, targets = zip(*batch)\n",
    "    \n",
    "    # Pad sequences in the batch to the maximum length\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=-1)\n",
    "    \n",
    "    return inputs_padded, targets_padded\n",
    "\n",
    "# def pad_collate_fn(batch):\n",
    "#     \"\"\"\n",
    "#     Pads the Mel spectrograms in a batch to have the same time dimension.\n",
    "#     \"\"\"\n",
    "#     # Get the maximum length of the time dimension in the batch\n",
    "#     max_len = max([mel_spec[0].shape[2] for mel_spec in batch])\n",
    "\n",
    "#     # Pad each mel spectrogram to the max length in the batch\n",
    "#     padded_batch = []\n",
    "#     label_batch = []\n",
    "#     for mel_spec in batch:\n",
    "#         # Pad along the time dimension (dim=2)\n",
    "#         pad_amount = max_len - mel_spec[0].shape[2]\n",
    "#         padded_mel_spec = torch.nn.functional.pad(mel_spec[0], (0, pad_amount), mode='constant', value=0)\n",
    "#         padded_mel_spec = padded_mel_spec.squeeze()\n",
    "#         padded_batch.append(padded_mel_spec)\n",
    "#         label_batch.append(mel_spec[1])\n",
    "\n",
    "\n",
    "#     # Stack the batch into a tensor\n",
    "#     return torch.stack(padded_batch),torch.Tensor(label_batch)\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     mel_specs, labels = zip(*batch)\n",
    "#     print(mel_specs)\n",
    "    \n",
    "#     # Find the max time dimension for padding\n",
    "#     max_len = max([mel_specs.shape[2] for mel_spec in batch])\n",
    "    \n",
    "#     # Pad all Mel spectrograms to the same length\n",
    "#     mel_specs_padded = [torch.nn.functional.pad(mel, (0, max_len - mel.shape[2])) for mel in mel_specs]\n",
    "    \n",
    "#     mel_specs_padded = torch.stack(mel_specs_padded)  # (batch_size, n_mels, max_len)\n",
    "#     labels = torch.tensor(labels)  # (batch_size,)\n",
    "    \n",
    "#     return mel_specs_padded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libri_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)\n",
    "\n",
    "# # Test the DataLoader again\n",
    "# for i, mel_spectrogram_batch in enumerate(libri_dataloader):\n",
    "#     print(f\"Batch {i+1}\")\n",
    "#     # print(f\"Mel Spectrogram Batch Shape: {mel_spectrogram_batch[1]}\")\n",
    "#     print(mel_spectrogram_batch[1].shape)\n",
    "    \n",
    "#     # Break after a few batches to avoid printing everything\n",
    "#     if i == 2:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_pairs_with_labels(embeddings, speaker_labels):\n",
    "    \"\"\"\n",
    "    Generate positive and negative pairs for contrastive learning based on speaker labels.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (torch.Tensor): The feature embeddings of shape (batch_size, embedding_dim).\n",
    "        speaker_labels (torch.Tensor): Tensor of speaker labels of shape (batch_size).\n",
    "        \n",
    "    Returns:\n",
    "        tuple of (positive_pairs, negative_pairs): Indices for positive and negative pairs.\n",
    "    \"\"\"\n",
    "    positive_pairs = []\n",
    "    negative_pairs = []\n",
    "    \n",
    "    # Iterate through each possible pair in the batch\n",
    "    for i in range(len(speaker_labels)):\n",
    "        for j in range(i + 1, len(speaker_labels)):\n",
    "            if speaker_labels[i] == speaker_labels[j]:\n",
    "                # Positive pair (same speaker)\n",
    "                positive_pairs.append((i, j))\n",
    "            else:\n",
    "                # Negative pair (different speakers)\n",
    "                negative_pairs.append((i, j))\n",
    "    \n",
    "    return positive_pairs, negative_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_contrastive_loss(embeddings, positive_pairs, negative_pairs, margin=1.0):\n",
    "    \"\"\"\n",
    "    Compute contrastive loss given the embeddings, positive pairs, and negative pairs.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (torch.Tensor): The feature embeddings of shape (batch_size, embedding_dim).\n",
    "        positive_pairs (list): List of tuples containing indices of positive pairs.\n",
    "        negative_pairs (list): List of tuples containing indices of negative pairs.\n",
    "        margin (float): The margin for contrastive loss.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The computed contrastive loss.\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    for (i, j) in positive_pairs:\n",
    "        # Positive pairs: minimize the distance\n",
    "        distance = torch.nn.functional.pairwise_distance(embeddings[i], embeddings[j])\n",
    "        loss += distance.pow(2)  # Minimize positive distance (same speaker)\n",
    "\n",
    "    for (i, j) in negative_pairs:\n",
    "        # Negative pairs: maximize the distance up to the margin\n",
    "        distance = torch.nn.functional.pairwise_distance(embeddings[i], embeddings[j])\n",
    "        loss += torch.relu(margin - distance).pow(2)  # Maximize negative distance (different speakers)\n",
    "\n",
    "    return loss / (len(positive_pairs) + len(negative_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i, mel_spectrogram_batch in enumerate(libri_dataloader):\n",
    "#     print(f\"Batch {i+1}\")\n",
    "#     embeddings = torch.randn(64, 256)\n",
    "#     # print(f\"Mel Spectrogram Batch Shape: {mel_spectrogram_batch[1]}\")\n",
    "#     # print(mel_spectrogram_batch[1].shape)\n",
    "#     positive_pairs, negative_pairs = generate_pairs_with_labels(embeddings, mel_spectrogram_batch[1])\n",
    "#     print(len(positive_pairs),len(negative_pairs))\n",
    "#     loss = compute_contrastive_loss(embeddings, positive_pairs, negative_pairs, margin=1.0)\n",
    "#     print(loss)\n",
    "#     # Break after a few batches to avoid printing everything\n",
    "#     if i == 2:\n",
    "#         break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_masked_indices(sequence_length, mask_prob=0.15):\n",
    "#     \"\"\"\n",
    "#     Create masked indices for the MLM task.\n",
    "    \n",
    "#     Args:\n",
    "#         sequence_length (int): Length of the sequence to mask.\n",
    "#         mask_prob (float): Probability of masking a position.\n",
    "    \n",
    "#     Returns:\n",
    "#         torch.Tensor: A tensor of masked indices (1 for masked, 0 for unmasked).\n",
    "#     \"\"\"\n",
    "#     # Generate a mask of the same length as the sequence with 1 for masked positions\n",
    "#     mask = torch.bernoulli(torch.full((sequence_length,), mask_prob)).bool()\n",
    "#     return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_mask_to_embeddings(embeddings, mask_indices, mask_token=None):\n",
    "#     \"\"\"\n",
    "#     Apply masking to the embeddings based on the mask indices.\n",
    "    \n",
    "#     Args:\n",
    "#         embeddings (torch.Tensor): The input embeddings (batch_size, sequence_length, embedding_dim).\n",
    "#         mask_indices (torch.Tensor): Boolean mask indicating which positions to mask.\n",
    "#         mask_token (torch.Tensor): The token to replace masked positions (default: zero tensor).\n",
    "    \n",
    "#     Returns:\n",
    "#         torch.Tensor: Masked embeddings.\n",
    "#     \"\"\"\n",
    "#     if mask_token is None:\n",
    "#         # Default mask token is a tensor of zeros with the same dimension as each embedding\n",
    "#         mask_token = torch.zeros(embeddings.size(-1))\n",
    "\n",
    "#     # Clone the embeddings to avoid modifying the original\n",
    "#     masked_embeddings =                     \n",
    "#     # Apply mask token to the masked positions\n",
    "#     masked_embeddings[mask_indices] = mask_token\n",
    "    \n",
    "\n",
    "\n",
    "#     return masked_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_indices(sequence_length, mask_prob=0.15):\n",
    "    # Generate random mask indices for the sequence length\n",
    "    mask = torch.rand(sequence_length) < mask_prob\n",
    "    return mask\n",
    "\n",
    "def apply_mask_to_embeddings(mel_specs, mask_indices, mask_token_value=0.0):\n",
    "    \"\"\"\n",
    "    Apply a mask to the mel spectrogram embeddings across the sequence length for each sample in the batch.\n",
    "    \"\"\"\n",
    "    # mel_specs: [batch_size, num_mel_bands, sequence_length]\n",
    "    batch_size, num_mel_bands, sequence_length = mel_specs.shape\n",
    "\n",
    "    # Expand the mask for the batch size and mel bands\n",
    "    mask_indices_expanded = mask_indices.unsqueeze(0).unsqueeze(1).expand(batch_size, num_mel_bands, -1)\n",
    "    \n",
    "    # Clone the mel_specs and apply the mask token value\n",
    "    masked_mel_specs = mel_specs.clone()\n",
    "    masked_mel_specs[mask_indices_expanded] = mask_token_value\n",
    "    \n",
    "    return masked_mel_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_language_modeling_loss( masked_mel_specs,original_embeddings, model, mask_indices,lengths):\n",
    "    \"\"\"\n",
    "    Compute MLM loss for audio embeddings.\n",
    "    \n",
    "    Args:\n",
    "        original_embeddings (torch.Tensor): The original unmasked embeddings (batch_size, sequence_length, embedding_dim).\n",
    "        masked_embeddings (torch.Tensor): The masked embeddings (batch_size, sequence_length, embedding_dim).\n",
    "        model (nn.Module): The model to predict masked embeddings.\n",
    "        mask_indices (torch.Tensor): Boolean mask indicating which positions were masked.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The computed MLM loss.\n",
    "    \"\"\"\n",
    "    # Pass the masked embeddings through the model\n",
    "    # output_embeddings = model(masked_embeddings)\n",
    "    # lengths = torch.full((masked_mel_specs.shape[0],), masked_mel_specs.shape[2], dtype=torch.int32).cuda()\n",
    "    # output_embeddings, output_embeddings_length = model(\n",
    "    # audio_signal=masked_mel_specs,\n",
    "    # length=lengths\n",
    "    # )\n",
    "    model.eval()\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        output_embeddings, output_embeddings_length = model(\n",
    "            audio_signal=masked_mel_specs,\n",
    "            length=lengths\n",
    "        )\n",
    "    # Compute loss only on masked positions\n",
    "    # masked_original = original_embeddings[mask_indices]\n",
    "    # masked_output = output_embeddings[mask_indices]\n",
    "    \n",
    "    # Mean squared error for regression\n",
    "    mlm_loss = torch.nn.functional.mse_loss(output_embeddings, original_embeddings)\n",
    "    \n",
    "    return mlm_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-11 17:14:52 mixins:172] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-11 17:14:52 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /data/NeMo_ASR_SET/English/v2.0/train/tarred_audio_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20.0\n",
      "    min_duration: 0.1\n",
      "    shuffle_n: 2048\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths: /data/NeMo_ASR_SET/English/v2.0/train/audio__OP_0..4095_CL_.tar\n",
      "    \n",
      "[NeMo W 2024-10-11 17:14:52 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n",
      "[NeMo W 2024-10-11 17:14:52 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-11 17:14:52 features:289] PADDING: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-11 17:14:53 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/nemo/core/connectors/save_restore_connector.py:571: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "      return torch.load(model_weights, map_location='cpu')\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-11 17:14:53 save_restore_connector:249] Model EncDecCTCModelBPE was successfully restored from /home/keagan/.cache/huggingface/hub/models--nvidia--stt_en_conformer_ctc_large/snapshots/2c8326e4e43ae5b994612cfea3f3029818fb23c6/stt_en_conformer_ctc_large.nemo.\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\")\n",
    "asr_model = nemo_asr.models.EncDecCTCModel.from_pretrained(model_name=\"nvidia/stt_en_conformer_ctc_large\")\n",
    "conformer_encoder = asr_model.encoder.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 5e-5\n",
    "data_dir = '/home/keagan/Documents/projects/SelfVC/data/archive/'\n",
    "model_save_path = '/home/keagan/Documents/projects/SelfVC/models/conformer_encoder_v1.pth'\n",
    "log_file_path = '/home/keagan/Documents/projects/SelfVC/models/conformer_encoder_loss.txt'\n",
    "optimizer = AdamW(conformer_encoder.parameters(), lr=learning_rate, betas=(0.9, 0.99))\n",
    "epochs = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LibriTTSDataset(data_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# collate_fn=collate_fn\n",
    "# collate_fn=pad_collate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m conformer_encoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mel_specs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      5\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      6\u001b[0m     mel_specs \u001b[38;5;241m=\u001b[39m mel_specs\u001b[38;5;241m.\u001b[39mcuda()  \u001b[38;5;66;03m# (batch_size, n_mels, max_len)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[2], line 85\u001b[0m, in \u001b[0;36mLibriTTSDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     80\u001b[0m     waveform \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(waveform, (\u001b[38;5;241m0\u001b[39m, padding))\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# return waveform\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Convert to mel spectrogram\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m mel_spectrogram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmel_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m mel_spectrogram \u001b[38;5;241m=\u001b[39m mel_spectrogram\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# print(f\"speaker to label {mel_spectrogram.shape}\")\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Return the mel spectrogram and the corresponding speaker label\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:619\u001b[0m, in \u001b[0;36mMelSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    612\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;124;03m        Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 619\u001b[0m     specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     mel_specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmel_scale(specgram)\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mel_specgram\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:110\u001b[0m, in \u001b[0;36mSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        Fourier bins, and time is the number of window hops (n_frame).\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torchaudio/functional/functional.py:126\u001b[0m, in \u001b[0;36mspectrogram\u001b[0;34m(waveform, pad, window, n_fft, hop_length, win_length, power, normalized, center, pad_mode, onesided, return_complex)\u001b[0m\n\u001b[1;32m    123\u001b[0m waveform \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# default values are consistent with librosa.core.spectrum._spectrogram\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m spec_f \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_length_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43monesided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# unpack batch\u001b[39;00m\n\u001b[1;32m    140\u001b[0m spec_f \u001b[38;5;241m=\u001b[39m spec_f\u001b[38;5;241m.\u001b[39mreshape(shape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m spec_f\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n",
      "File \u001b[0;32m~/anaconda3/envs/vc/lib/python3.10/site-packages/torch/functional.py:666\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex)\u001b[0m\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(extended_shape), [pad, pad], pad_mode)\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39msignal_dim:])\n\u001b[0;32m--> 666\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):  # Train for 50 epochs\n",
    "    conformer_encoder.train()\n",
    "    total_loss = 0\n",
    "    for mel_specs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        mel_specs = mel_specs.cuda()  # (batch_size, n_mels, max_len)\n",
    "        labels = labels  # (batch_size,)\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        # embeddings = conformer_encoder(mel_specs)  # (batch_size, output_dim, max_len)\n",
    "        \n",
    "        # print(mel_specs.shape[2])\n",
    "        # print(torch.tensor(mel_specs.shape[2]))\n",
    "        lengths = torch.full((mel_specs.shape[0],), mel_specs.shape[2], dtype=torch.int32).cuda()\n",
    "        # print(lengths.shape)\n",
    "        # with autocast():\n",
    "        embeddings, embeddings_length = conformer_encoder(\n",
    "            audio_signal=mel_specs,\n",
    "            length=lengths\n",
    "        )\n",
    "        # print(\"Test\")\n",
    "        # Contrastive loss\n",
    "        positive_pairs, negative_pairs = generate_pairs_with_labels(embeddings, labels)\n",
    "        contrastive_loss = compute_contrastive_loss(embeddings, positive_pairs, negative_pairs)\n",
    "        contrastive_loss = contrastive_loss.mean()\n",
    "        # MLM loss (masked positions)\n",
    "        mask_indices = create_masked_indices(sequence_length=mel_specs.shape[2])\n",
    "        masked_mel_specs = apply_mask_to_embeddings(mel_specs, mask_indices)\n",
    "        # masked_mel_specs = apply_mask_to_embeddings(mel_specs, mask_indices)\n",
    "        mlm_loss = masked_language_modeling_loss(masked_mel_specs, embeddings,conformer_encoder, mask_indices,lengths)\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = contrastive_loss + mlm_loss\n",
    "        total_loss += loss.item()\n",
    "        # print(contrastive_loss,mlm_loss)\n",
    "        with open(log_file_path, 'a') as log_file:\n",
    "            log_file.write(f'Contrastive Loss: {contrastive_loss:.6f} MLM Loss: {mlm_loss:.6f}\\n')\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # scaler.scale(loss).backward()\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\\n')    \n",
    "    torch.save(conformer_encoder, model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
