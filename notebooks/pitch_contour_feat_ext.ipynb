{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "# import torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized Pitch Contour Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_f0(audio_path, sr=22050):\n",
    "    \"\"\"\n",
    "    Extract the fundamental frequency (F0) contour using PYin algorithm.\n",
    "    \n",
    "    Args:\n",
    "    - audio_path (str): Path to the audio file.\n",
    "    - sr (int): Sampling rate. Default is 22050.\n",
    "    \n",
    "    Returns:\n",
    "    - f0_contour (np.ndarray): Extracted F0 contour.\n",
    "    - times (np.ndarray): Time axis corresponding to the F0 contour.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_path, sr=sr)\n",
    "    \n",
    "    # Compute the F0 (fundamental frequency) using PYin\n",
    "    f0_contour, voiced_flag, voiced_probs = librosa.pyin(\n",
    "        y, \n",
    "        fmin=librosa.note_to_hz('C2'),  # Minimum pitch (in Hz)\n",
    "        fmax=librosa.note_to_hz('C7')   # Maximum pitch (in Hz)\n",
    "    )\n",
    "    print(f0_contour.shape)\n",
    "    # Replace unvoiced frames (None) with zeros or some placeholder\n",
    "    f0_contour = np.nan_to_num(f0_contour)\n",
    "\n",
    "    # Generate time axis for plotting\n",
    "    # times = librosa.times_like(f0_contour, sr=sr)\n",
    "    \n",
    "    return f0_contour\n",
    "\n",
    "# Function to normalize the F0 contour\n",
    "def normalize_f0(f0_contour):\n",
    "    \"\"\"\n",
    "    Normalize the F0 contour using its mean and standard deviation.\n",
    "    Excludes unvoiced frames (zeros or NaNs) from calculation.\n",
    "    \n",
    "    Args:\n",
    "    - f0_contour (np.ndarray): F0 contour to normalize.\n",
    "    \n",
    "    Returns:\n",
    "    - normalized_f0 (np.ndarray): Normalized F0 contour.\n",
    "    - mean_f0 (float): Mean of the original F0 contour (excluding unvoiced).\n",
    "    - std_f0 (float): Standard deviation of the original F0 contour (excluding unvoiced).\n",
    "    \"\"\"\n",
    "    # Filter out unvoiced (NaN or 0 values)\n",
    "    # voiced_f0 = f0_contour[f0_contour > 0]  # Exclude unvoiced frames\n",
    "    \n",
    "    # Compute mean and standard deviation only for voiced frames\n",
    "    mean_f0 = np.mean(f0_contour)\n",
    "    std_f0 = np.std(f0_contour)\n",
    "    \n",
    "    # Normalize F0 contour (keep NaNs for unvoiced frames)\n",
    "    normalized_f0 = (f0_contour - mean_f0) / std_f0\n",
    "\n",
    "    # Optionally: Clip or apply ReLU to remove negative values (if required)\n",
    "    # normalized_f0 = np.clip(normalized_f0, 0, None)\n",
    "    \n",
    "    return normalized_f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(389,)\n"
     ]
    }
   ],
   "source": [
    "audio_path = '/home/keagan/Documents/projects/SelfVC/data/audios/14_208_000001_000000.wav'\n",
    "f0 = extract_f0(audio_path, sr=22050)\n",
    "normalized_f0 = normalize_f0(f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "waveform, sr = torchaudio.load(audio_path)\n",
    "resample_transform = T.Resample(orig_freq=sr, new_freq=22050)\n",
    "mel_transform = T.MelSpectrogram(\n",
    "    sample_rate=22050,\n",
    "    n_mels=80,\n",
    "    n_fft=1024,\n",
    "    hop_length=256,\n",
    "    win_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_waveform = resample_transform(waveform)\n",
    "mel_spectro = mel_transform(resampled_waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 777])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_spectro.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(389,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_f0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(389,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
