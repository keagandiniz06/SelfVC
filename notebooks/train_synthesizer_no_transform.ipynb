{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-21 11:51:17 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:254: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(\n",
      "    \n",
      "[NeMo W 2024-10-21 11:51:17 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:265: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-10-21 11:51:17 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:325: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(\n",
      "    \n",
      "[NeMo W 2024-10-21 11:51:17 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:360: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torch.nn as nn\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import json\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_embeddings(mel_spectrogram_tensor, encoder,device):\n",
    "    mel_tensor = mel_spectrogram_tensor.to(device)\n",
    "    lengths = torch.full((mel_tensor.shape[0],), mel_tensor.shape[2], dtype=torch.int32).to(device)\n",
    "\n",
    "    # # Ensure the input tensor has the correct shape\n",
    "    # if len(mel_tensor.shape) == 2:\n",
    "    #     # Adding batch dimension (1, mel_channels, time_frames)\n",
    "    #     mel_tensor = mel_tensor.unsqueeze(0)\n",
    "\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        # content_embeddings = encoder(mel_tensor)\n",
    "        content_embeddings, _ = encoder(\n",
    "            audio_signal=mel_tensor,\n",
    "            length=lengths\n",
    "        )        \n",
    "\n",
    "    return content_embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duration Augmented Content Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    return F.cosine_similarity(v1.unsqueeze(0), v2.unsqueeze(0), dim=-1).item()\n",
    "\n",
    "def group_similar_vectors(vectors, threshold, vector_duration):\n",
    "    grouped_vectors = []\n",
    "    durations = []\n",
    "\n",
    "    current_group = [vectors[:, 0]]  # Start with the first vector (slice across batch dimension)\n",
    "    current_duration = vector_duration\n",
    "\n",
    "    for i in range(1, vectors.shape[-1]):\n",
    "        sim = cosine_similarity(vectors[:, i], vectors[:, i - 1])\n",
    "        # print(sim)\n",
    "        # If cosine similarity is above threshold, group the vectors\n",
    "        if sim > threshold:\n",
    "            # print(\"Exceeded Threshold\")\n",
    "            current_group.append(vectors[:, i])\n",
    "            current_duration += vector_duration\n",
    "        else:\n",
    "            # Compute the average of the current group and save it\n",
    "            averaged_vector = torch.mean(torch.stack(current_group, dim=-1), dim=-1)\n",
    "            grouped_vectors.append(averaged_vector)\n",
    "            durations.append(current_duration)\n",
    "\n",
    "            # Start a new group\n",
    "            current_group = [vectors[:, i]]\n",
    "            current_duration = vector_duration\n",
    "\n",
    "    # Append the last group\n",
    "    if current_group:\n",
    "        averaged_vector = torch.mean(torch.stack(current_group, dim=-1), dim=-1)\n",
    "        grouped_vectors.append(averaged_vector)\n",
    "        durations.append(current_duration)\n",
    "\n",
    "    return torch.stack(grouped_vectors, dim=-1), durations\n",
    "\n",
    "\n",
    "def duration_augmented_representation(content_embeddings, T=0.925, vector_duration=46.44):\n",
    "    # Remove the batch dimension for processing\n",
    "    content_vectors = content_embeddings.squeeze(0)  # Shape: [256, num_content_vectors]\n",
    "\n",
    "    grouped_vectors, new_durations = group_similar_vectors(content_vectors, T, vector_duration)\n",
    "\n",
    "    # Convert durations to seconds if needed\n",
    "    new_durations_in_seconds = [d / 1000 for d in new_durations]\n",
    "\n",
    "    # Add the batch dimension back to the output\n",
    "    grouped_vectors = grouped_vectors.unsqueeze(0)  # Shape: [1, 256, num_grouped_vectors]\n",
    "\n",
    "    return grouped_vectors, new_durations_in_seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitch Contour Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_f0(audio_path, sr=22050):\n",
    "    \"\"\"\n",
    "    Extract the fundamental frequency (F0) contour using PYin algorithm.\n",
    "    \n",
    "    Args:\n",
    "    - audio_path (str): Path to the audio file.\n",
    "    - sr (int): Sampling rate. Default is 22050.\n",
    "    \n",
    "    Returns:\n",
    "    - f0_tensor (torch.Tensor): Extracted F0 contour as a PyTorch tensor.\n",
    "    - pitch_tensor (torch.Tensor): Extracted pitch contour as a PyTorch tensor.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_path, sr=sr)\n",
    "\n",
    "    # Compute the F0 (fundamental frequency) using PYin\n",
    "    f0_contour, voiced_flag, voiced_probs = librosa.pyin(\n",
    "        y, \n",
    "        fmin=librosa.note_to_hz('C2'),  # Minimum pitch (in Hz)\n",
    "        fmax=librosa.note_to_hz('C7')   # Maximum pitch (in Hz)\n",
    "    )\n",
    "    \n",
    "    # Replace unvoiced frames (None) with zeros or some placeholder\n",
    "    f0_contour = np.nan_to_num(f0_contour)\n",
    "    # print(\"reahced here\")\n",
    "    # Convert to PyTorch tensor\n",
    "    f0_tensor = torch.tensor(f0_contour, dtype=torch.float32)\n",
    "    # print(\"reahced here 1\")\n",
    "    # Optionally, if you want to extract a pitch contour (e.g., using voiced probabilities)\n",
    "    # pitch_tensor = torch.tensor(voiced_probs, dtype=torch.float32)  # Using voiced probabilities as pitch contour\n",
    "\n",
    "    return f0_tensor\n",
    "\n",
    "\n",
    "# Function to normalize the F0 contour\n",
    "def normalize_f0(f0_contour):\n",
    "\n",
    "    # Filter out unvoiced (NaN or 0 values)\n",
    "    # voiced_f0 = f0_contour[f0_contour > 0]  # Exclude unvoiced frames\n",
    "    \n",
    "    # Compute mean and standard deviation only for voiced frames\n",
    "    mean_f0 = np.mean(f0_contour)\n",
    "    std_f0 = np.std(f0_contour)\n",
    "    \n",
    "    # Normalize F0 contour (keep NaNs for unvoiced frames)\n",
    "    normalized_f0 = (f0_contour - mean_f0) / std_f0\n",
    "\n",
    "    # Optionally: Clip or apply ReLU to remove negative values (if required)\n",
    "    # normalized_f0 = np.clip(normalized_f0, 0, None)\n",
    "    \n",
    "    return normalized_f0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_model = nemo_asr.models.EncDecSpeakerLabelModel.from_pretrained(model_name='titanet_large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynthesizerNoTransformDataset(Dataset):\n",
    "    def __init__(self, data,root_dir,content_encoder,speaker_encoder, device):\n",
    "        # Load the JSON file\n",
    "        # with open(json_file, 'r') as f:\n",
    "        self.data = data\n",
    "        self.content_encoder = content_encoder\n",
    "        self.speaker_encoder = speaker_encoder\n",
    "        self.device = device \n",
    "        self.root_dir = root_dir           \n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return the number of entries in the dataset\n",
    "        return len(self.data)\n",
    "    \n",
    "    def extract_content_embeddings(self, mel_spectrogram_tensor):\n",
    "        mel_tensor = mel_spectrogram_tensor.unsqueeze(0).to(self.device)  # Add batch dimension\n",
    "        lengths = torch.full((mel_tensor.shape[0],), mel_tensor.shape[2], dtype=torch.int32).to(self.device)\n",
    "\n",
    "        self.content_encoder.eval()\n",
    "        with torch.no_grad():\n",
    "            content_embeddings, _ = self.content_encoder(\n",
    "                audio_signal=mel_tensor,\n",
    "                length=lengths\n",
    "            )\n",
    "\n",
    "        return content_embeddings.cpu()   \n",
    "    \n",
    "    def extract_speaker_embeddings(self, audio_path):\n",
    "        self.speaker_encoder.eval()\n",
    "        with torch.no_grad():\n",
    "            speaker_embedding = self.speaker_encoder.get_embedding(audio_path).cpu()\n",
    "        return speaker_embedding      \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        mel_path =  self.root_dir + \"/mel_spectrograms/\" + self.data[idx]['mel_filepath']\n",
    "        audio_path = self.root_dir + \"/audios/\" + self.data[idx]['audio_filepath']\n",
    "        # print(audio_path)\n",
    "        mel_spectrogram = torch.from_numpy(np.load(mel_path))\n",
    "        content_embeddings = self.extract_content_embeddings(mel_spectrogram) \n",
    "        speaker_embeddings = self.extract_speaker_embeddings(audio_path)  \n",
    "        # duration_augmented_content_embeddings, durations = self.duration_augmented_representation(content_embeddings)\n",
    "        # print(duration_augmented_content_embeddings.shape)\n",
    "        # print(\"Reached herte amin\")\n",
    "        # normalized_pitch_contour =self.normalize_f0(audio_path)\n",
    "        # print(normalized_pitch_contour.shape)\n",
    "        \n",
    "        return content_embeddings.squeeze(),speaker_embeddings.squeeze(),audio_path\n",
    "    # ,duration_augmented_content_embeddings\n",
    "    # ,normalized_pitch_contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(batch):\n",
    "#     \"\"\"\n",
    "#     Pads the content embeddings, speaker embeddings, and duration-augmented content embeddings in the batch to the same length.\n",
    "    \n",
    "#     Args:\n",
    "#     - batch (list of tuples): Each tuple contains content_embeddings, speaker_embeddings, duration_augmented_content_embeddings.\n",
    "    \n",
    "#     Returns:\n",
    "#     - padded_content_embeddings (torch.Tensor): Padded content embeddings.\n",
    "#     - speaker_batch (torch.Tensor): Speaker embeddings.\n",
    "#     - padded_duration_augmented_embeddings (torch.Tensor): Padded duration-augmented content embeddings.\n",
    "#     \"\"\"\n",
    "#     # Unzip the batch into separate components\n",
    "#     content_embeddings, speaker_embeddings,audio_path = zip(*batch)\n",
    "    \n",
    "#     # Find the maximum length in the batch for padding\n",
    "#     max_len_content = max([embedding.size(-1) for embedding in content_embeddings])\n",
    "#     # max_len_duration = max([embedding.size(-1) for embedding in duration_augmented_content_embeddings])\n",
    "    \n",
    "#     # Pad the content embeddings to the maximum length\n",
    "#     padded_content_embeddings = [F.pad(embedding, (0, max_len_content - embedding.size(-1))) for embedding in content_embeddings]\n",
    "    \n",
    "#     # Pad the duration-augmented content embeddings to the maximum length\n",
    "#     # padded_duration_augmented_embeddings = [F.pad(embedding, (0, max_len_duration - embedding.size(-1))) for embedding in duration_augmented_content_embeddings]\n",
    "\n",
    "#     # Stack the speaker embeddings (assuming they are already of fixed size)\n",
    "#     speaker_batch = torch.stack(speaker_embeddings)\n",
    "#     # duration_augmented_content_batch = torch.stack(duration_augmented_content_embeddings)\n",
    "#     # normalized_pitch_contour_batch = torch.stack(normalized_pitch_contour)\n",
    "    \n",
    "#     # Stack the padded embeddings\n",
    "#     padded_content_embeddings = torch.stack(padded_content_embeddings)\n",
    "#     # f0_tensors_padded = pad_sequence(normalized_pitch_contour, batch_first=True, padding_value=0.0)\n",
    "#     # padded_duration_augmented_embeddings = torch.stack(padded_duration_augmented_embeddings)\n",
    "\n",
    "#     return padded_content_embeddings, speaker_batch,audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads the content embeddings, speaker embeddings, and duration-augmented content embeddings in the batch to the same length.\n",
    "    \n",
    "    Args:\n",
    "    - batch (list of tuples): Each tuple contains content_embeddings, speaker_embeddings, audio_path.\n",
    "    \n",
    "    Returns:\n",
    "    - padded_content_embeddings (torch.Tensor): Padded content embeddings.\n",
    "    - speaker_batch (torch.Tensor): Speaker embeddings.\n",
    "    - audio_path (list): Audio paths from the batch.\n",
    "    - padding_mask (torch.Tensor): Mask indicating valid entries.\n",
    "    \"\"\"\n",
    "    # Unzip the batch into separate components\n",
    "    content_embeddings, speaker_embeddings, audio_path = zip(*batch)\n",
    "    \n",
    "    # Find the maximum length in the batch for padding\n",
    "    max_len_content = max([embedding.size(-1) for embedding in content_embeddings])\n",
    "    \n",
    "    # Pad the content embeddings to the maximum length\n",
    "    padded_content_embeddings = [F.pad(embedding, (0, max_len_content - embedding.size(-1))) for embedding in content_embeddings]\n",
    "    \n",
    "    # Stack the speaker embeddings (assuming they are already of fixed size)\n",
    "    speaker_batch = torch.stack(speaker_embeddings)\n",
    "    \n",
    "    # Stack the padded content embeddings\n",
    "    padded_content_embeddings = torch.stack(padded_content_embeddings)\n",
    "\n",
    "    # Create a padding mask: 1 for valid entries, 0 for padding\n",
    "    padding_mask = (padded_content_embeddings != 0).float()  # Assuming 0 is the padding value\n",
    "\n",
    "    return padded_content_embeddings, speaker_batch, audio_path, padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/media/keagan/hdd/project_data/SelfVC/data/val_filelist_clean.json\"\n",
    "with open(data_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerEncoder256(nn.Module):\n",
    "    def __init__(self, original_encoder):\n",
    "        super(ConformerEncoder256, self).__init__()\n",
    "        self.encoder = original_encoder\n",
    "        # Add a 1D convolution with stride to downsample to 256\n",
    "        self.downsample = nn.Conv1d(in_channels=512, out_channels=256, kernel_size=1, stride=2) # Example\n",
    "\n",
    "    def forward(self, audio_signal, length):\n",
    "        # Pass through original encoder\n",
    "        embeddings, lengths = self.encoder(audio_signal=audio_signal, length=length)\n",
    "        # Reduce the embedding size\n",
    "        embeddings = self.downsample(embeddings)\n",
    "        return embeddings, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-21 11:52:21 nemo_logging:349] /tmp/ipykernel_76375/3154154602.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "      conformer_encoder_256 = torch.load('/home/keagan/Documents/projects/SelfVC/models/conformer_encoder_v2.pth')\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-21 11:52:21 cloud:58] Found existing object /home/keagan/.cache/torch/NeMo/NeMo_1.23.0/titanet-l/11ba0924fdf87c049e339adbf6899d48/titanet-l.nemo.\n",
      "[NeMo I 2024-10-21 11:52:21 cloud:64] Re-using file from: /home/keagan/.cache/torch/NeMo/NeMo_1.23.0/titanet-l/11ba0924fdf87c049e339adbf6899d48/titanet-l.nemo\n",
      "[NeMo I 2024-10-21 11:52:21 common:924] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-21 11:52:21 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /manifests/combined_fisher_swbd_voxceleb12_librispeech/train.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    augmentor:\n",
      "      noise:\n",
      "        manifest_path: /manifests/noise/rir_noise_manifest.json\n",
      "        prob: 0.5\n",
      "        min_snr_db: 0\n",
      "        max_snr_db: 15\n",
      "      speed:\n",
      "        prob: 0.5\n",
      "        sr: 16000\n",
      "        resample_type: kaiser_fast\n",
      "        min_speed_rate: 0.95\n",
      "        max_speed_rate: 1.05\n",
      "    num_workers: 15\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2024-10-21 11:52:21 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /manifests/combined_fisher_swbd_voxceleb12_librispeech/dev.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    num_workers: 15\n",
      "    pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-21 11:52:21 features:289] PADDING: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-21 11:52:22 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/nemo/core/connectors/save_restore_connector.py:571: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "      return torch.load(model_weights, map_location='cpu')\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-21 11:52:22 save_restore_connector:249] Model EncDecSpeakerLabelModel was successfully restored from /home/keagan/.cache/torch/NeMo/NeMo_1.23.0/titanet-l/11ba0924fdf87c049e339adbf6899d48/titanet-l.nemo.\n"
     ]
    }
   ],
   "source": [
    "conformer_encoder_256 = torch.load('/home/keagan/Documents/projects/SelfVC/models/conformer_encoder_v2.pth')\n",
    "speaker_model = nemo_asr.models.EncDecSpeakerLabelModel.from_pretrained(model_name='titanet_large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "root_dir = \"/media/keagan/hdd/project_data/SelfVC/data\"\n",
    "dataset = SynthesizerNoTransformDataset(data,root_dir,conformer_encoder_256,speaker_model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4  # Adjust as needed\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-21 12:00:31 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/nemo/collections/asr/parts/preprocessing/features.py:417: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "      with torch.cuda.amp.autocast(enabled=False):\n",
      "    \n",
      "[NeMo W 2024-10-21 12:00:31 nemo_logging:349] /home/keagan/anaconda3/envs/vc/lib/python3.10/site-packages/nemo/collections/asr/parts/submodules/jasper.py:476: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "      with torch.cuda.amp.autocast(enabled=False):\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "content_embeddings Shape: torch.Size([4, 256, 199])\n",
      "speaker_embeddings Shape: torch.Size([4, 192])\n",
      "audio_path: ('/media/keagan/hdd/project_data/SelfVC/data/audios/2787_157400_000050_000000.wav', '/media/keagan/hdd/project_data/SelfVC/data/audios/6918_61317_000026_000000.wav', '/media/keagan/hdd/project_data/SelfVC/data/audios/2960_155152_000016_000003.wav', '/media/keagan/hdd/project_data/SelfVC/data/audios/3307_145145_000050_000005.wav')\n",
      "padding_mask: torch.Size([4, 256, 199])\n"
     ]
    }
   ],
   "source": [
    "for i, (content_embeddings,speaker_embeddings,audio_path,padding_mask) in enumerate(dataloader):\n",
    "    # duration_augmented_content_embeddings\n",
    "    # normalized_pitch_contour\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(f\"content_embeddings Shape: {content_embeddings.shape}\")\n",
    "    print(f\"speaker_embeddings Shape: {speaker_embeddings.shape}\")\n",
    "    print(f\"audio_path: {audio_path}\")\n",
    "    print(f\"padding_mask: {padding_mask.shape}\")\n",
    "    # print(f\"duration_augmented_content_embeddings Shape: {duration_augmented_content_embeddings.shape}\")\n",
    "    # print(f\"normalized_pitch_contour Shape: {normalized_pitch_contour.shape}\")\n",
    "    # Optionally, break after first batch for testing\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 199])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reahced here\n",
      "reahced here 1\n"
     ]
    }
   ],
   "source": [
    "f0_tensor = extract_f0(\"/home/keagan/Documents/projects/SelfVC/data/audios/4957_36386_000058_000002.wav\", sr=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000, 174.6141, 169.6432,\n",
       "        167.6947, 165.7685, 161.0494, 158.2827, 169.6432, 220.0000, 245.5194,\n",
       "        261.6256, 278.7883, 285.3047, 283.6615, 257.1310, 216.2205, 258.6205,\n",
       "        245.5194, 233.0819, 223.8455, 220.0000, 220.0000, 222.5563, 225.1423,\n",
       "        248.3722, 246.9417, 248.3722, 252.7136, 260.1187, 264.6655, 260.1187,\n",
       "          0.0000,   0.0000, 220.0000, 211.2820, 204.0850, 194.8689, 190.4180,\n",
       "        191.5211, 197.1331, 197.1331, 186.0689, 211.2820, 210.0652, 204.0850,\n",
       "        202.9096, 202.9096, 207.6523, 207.6523,   0.0000,   0.0000, 248.3722,\n",
       "        249.8110, 252.7136, 261.6256, 269.2918, 277.1826, 283.6615,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000, 208.8553, 231.7394, 249.8110, 261.6256, 264.6655, 266.1987,\n",
       "        267.7408,   0.0000,   0.0000, 239.9117, 223.8455, 213.7370, 210.0652,\n",
       "        204.0850, 182.8723, 178.6955, 170.6259, 157.3710, 144.3099,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f0_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_f0(audio_path, sr=22050):\n",
    "    \"\"\"\n",
    "    Normalize the F0 contour.\n",
    "    \n",
    "    Args:\n",
    "    - f0_tensor (torch.Tensor): F0 contour as a PyTorch tensor.\n",
    "    \n",
    "    Returns:\n",
    "    - normalized_f0 (torch.Tensor): Normalized F0 contour.\n",
    "    \"\"\"\n",
    "    f0_tensor = extract_f0(audio_path, sr=22050)\n",
    "    # Filter out unvoiced (NaN or 0 values)\n",
    "    # voiced_f0 = f0_tensor[f0_tensor > 0]  # Exclude unvoiced frames\n",
    "    \n",
    "    # Compute mean and standard deviation only for voiced frames\n",
    "    mean_f0 = f0_tensor.mean()\n",
    "    std_f0 = f0_tensor.std()\n",
    "    \n",
    "    # Normalize F0 contour (keep NaNs for unvoiced frames)\n",
    "    normalized_f0 = (f0_tensor - mean_f0) / std_f0\n",
    "\n",
    "    return normalized_f0     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reahced here\n",
      "reahced here 1\n"
     ]
    }
   ],
   "source": [
    "f0_tensor_norm = normalize_f0(\"/home/keagan/Documents/projects/SelfVC/data/audios/4957_36386_000058_000002.wav\", sr=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1538, -1.1538, -1.1538, -1.1538, -1.1538,  0.3828,  0.3390,  0.3219,\n",
       "         0.3049,  0.2634,  0.2391,  0.3390,  0.7822,  1.0067,  1.1485,  1.2995,\n",
       "         1.3568,  1.3424,  1.1089,  0.7489,  1.1220,  1.0067,  0.8973,  0.8160,\n",
       "         0.7822,  0.7822,  0.8047,  0.8274,  1.0318,  1.0193,  1.0318,  1.0700,\n",
       "         1.1352,  1.1752,  1.1352, -1.1538, -1.1538,  0.7822,  0.7055,  0.6421,\n",
       "         0.5610,  0.5218,  0.5316,  0.5809,  0.5809,  0.4836,  0.7055,  0.6947,\n",
       "         0.6421,  0.6318,  0.6318,  0.6735,  0.6735, -1.1538, -1.1538,  1.0318,\n",
       "         1.0445,  1.0700,  1.1485,  1.2159,  1.2854,  1.3424, -1.1538, -1.1538,\n",
       "        -1.1538, -1.1538, -1.1538, -1.1538, -1.1538, -1.1538, -1.1538, -1.1538,\n",
       "        -1.1538, -1.1538, -1.1538, -1.1538, -1.1538, -1.1538, -1.1538, -1.1538,\n",
       "        -1.1538, -1.1538, -1.1538, -1.1538, -1.1538, -1.1538, -1.1538, -1.1538,\n",
       "        -1.1538, -1.1538, -1.1538, -1.1538,  0.6841,  0.8855,  1.0445,  1.1485,\n",
       "         1.1752,  1.1887,  1.2023, -1.1538, -1.1538,  0.9574,  0.8160,  0.7271,\n",
       "         0.6947,  0.6421,  0.4554,  0.4187,  0.3477,  0.2310,  0.1161, -1.1538,\n",
       "        -1.1538, -1.1538, -1.1538, -1.1538, -1.1538, -1.1538, -1.1538])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f0_tensor_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionwiseConvFF(nn.Module):\n",
    "#     def __init__(self, d_model, d_inner, kernel_size, dropout, pre_lnorm=False):\n",
    "#         super(PositionwiseConvFF, self).__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.d_inner = d_inner\n",
    "#         self.dropout = dropout\n",
    "\n",
    "#         self.CoreNet = nn.Sequential(\n",
    "#             nn.Conv1d(d_model, d_inner, kernel_size, 1, (kernel_size // 2)),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(d_inner, d_model, kernel_size, 1, (kernel_size // 2)),\n",
    "#             nn.Dropout(dropout),\n",
    "#         )\n",
    "#         self.layer_norm = nn.LayerNorm(d_model)\n",
    "#         self.pre_lnorm = pre_lnorm\n",
    "\n",
    "#     def forward(self, inp):\n",
    "#         if self.pre_lnorm:\n",
    "#             core_out = inp.transpose(1, 2)\n",
    "#             core_out = self.CoreNet(self.layer_norm(core_out).to(inp.dtype))\n",
    "#             core_out = core_out.transpose(1, 2)\n",
    "#             output = core_out + inp\n",
    "#         else:\n",
    "#             core_out = inp.transpose(1, 2)\n",
    "#             core_out = self.CoreNet(core_out)\n",
    "#             core_out = core_out.transpose(1, 2)\n",
    "#             output = self.layer_norm(inp + core_out).to(inp.dtype)\n",
    "#         return output\n",
    "\n",
    "\n",
    "# class MultiHeadAttn(nn.Module):\n",
    "#     def __init__(self, n_head, d_model, d_head, dropout, dropatt=0.1, pre_lnorm=False):\n",
    "#         super(MultiHeadAttn, self).__init__()\n",
    "#         self.n_head = n_head\n",
    "#         self.d_model = d_model\n",
    "#         self.d_head = d_head\n",
    "#         self.scale = 1 / (d_head ** 0.5)\n",
    "#         self.pre_lnorm = pre_lnorm\n",
    "\n",
    "#         self.qkv_net = nn.Linear(d_model, 3 * n_head * d_head)\n",
    "#         # self.qkv_net = nn.Linear(d_model, n_head * d_head)\n",
    "#         self.drop = nn.Dropout(dropout)\n",
    "#         self.dropatt = nn.Dropout(dropatt)\n",
    "#         self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n",
    "#         self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "#     def forward(self, inp, attn_mask=None):\n",
    "#         # print(inp.shape)\n",
    "#         residual = inp\n",
    "#         if self.pre_lnorm:\n",
    "#             inp = self.layer_norm(inp)\n",
    "\n",
    "#         n_head, d_head = self.n_head, self.d_head\n",
    "#         # head_q, head_k, head_v = torch.chunk(self.qkv_net(inp), 3, dim=2)\n",
    "#         #b_size,512,time_steps\n",
    "#         inp_permuted = inp.permute(0, 2, 1)\n",
    "#         #b_size,time_steps,512\n",
    "#         if not None:\n",
    "#             attn_mask = attn_mask.permute(0, 2, 1)\n",
    "#             #b_size,time_steps,512\n",
    "#         # print(inp_permuted.shape)\n",
    "#         qkv_out = self.qkv_net(inp_permuted)\n",
    "#         #b_size,time_steps,192\n",
    "#         # print(qkv_out.shape)\n",
    "#         head_q, head_k, head_v = torch.chunk(qkv_out, 3, dim=2)\n",
    "#         #b_size,time_steps,64,b_size,time_steps,64,b_size,time_steps,64\n",
    "#         # print(head_q.shape,head_k.shape,head_v.shape)\n",
    "#         head_q = head_q.view(inp_permuted.size(0), inp_permuted.size(1), n_head, d_head)\n",
    "#         head_k = head_k.view(inp_permuted.size(0), inp_permuted.size(1), n_head, d_head)\n",
    "#         head_v = head_v.view(inp_permuted.size(0), inp_permuted.size(1), n_head, d_head)\n",
    "#         # print(head_q.shape,head_k.shape,head_v.shape)\n",
    "#         q = head_q.permute(2, 0, 1, 3).reshape(-1, inp_permuted.size(1), d_head)\n",
    "#         k = head_k.permute(2, 0, 1, 3).reshape(-1, inp_permuted.size(1), d_head)\n",
    "#         v = head_v.permute(2, 0, 1, 3).reshape(-1, inp_permuted.size(1), d_head)\n",
    "#         # print(q.shape,k.shape,v.shape)\n",
    "#         #4,145,64 * 4,64,145 = 4,145,145 \n",
    "#         attn_score = torch.bmm(q, k.transpose(1, 2))\n",
    "#         print(attn_score.shape)\n",
    "#         attn_score.mul_(self.scale)\n",
    "#         print(attn_score.shape)\n",
    "\n",
    "#         if attn_mask is not None:\n",
    "#             print(\"reached MultiHeadAttn 1\")\n",
    "#             # print(attn_mask.shape)\n",
    "#             # print(attn_mask.size(2))\n",
    "#             # attn_mask = attn_mask.unsqueeze(1).to(attn_score.dtype)\n",
    "#             attn_mask = attn_mask.repeat(n_head, attn_mask.size(1), 1)\n",
    "#             print(attn_mask.shape)\n",
    "#             print(attn_score.shape)\n",
    "#             attn_score.masked_fill_(attn_mask.to(torch.bool), -float('inf'))\n",
    "            \n",
    "\n",
    "#         attn_prob = torch.softmax(attn_score, dim=2)\n",
    "#         attn_prob = self.dropatt(attn_prob)\n",
    "#         attn_vec = torch.bmm(attn_prob, v)\n",
    "\n",
    "#         attn_vec = attn_vec.view(n_head, inp.size(0), inp.size(1), d_head)\n",
    "#         attn_vec = attn_vec.permute(1, 2, 0, 3).contiguous().view(\n",
    "#             inp.size(0), inp.size(1), n_head * d_head)\n",
    "\n",
    "#         attn_out = self.o_net(attn_vec)\n",
    "#         attn_out = self.drop(attn_out)\n",
    "\n",
    "#         if self.pre_lnorm:\n",
    "#             output = residual + attn_out\n",
    "#         else:\n",
    "#             output = self.layer_norm(residual + attn_out)\n",
    "\n",
    "#         return output\n",
    "\n",
    "\n",
    "# class TransformerLayer(nn.Module):\n",
    "#     def __init__(self, n_head, d_model, d_head, d_inner, kernel_size, dropout, **kwargs):\n",
    "#         super(TransformerLayer, self).__init__()\n",
    "#         self.dec_attn = MultiHeadAttn(n_head, d_model, d_head, dropout, **kwargs)\n",
    "#         self.pos_ff = PositionwiseConvFF(d_model, d_inner, kernel_size, dropout, pre_lnorm=kwargs.get('pre_lnorm'))\n",
    "\n",
    "#     def forward(self, dec_inp, mask=None):\n",
    "#         # print(dec_inp.shape)\n",
    "#         # print(mask.squeeze(2).shape)\n",
    "#         output = self.dec_attn(dec_inp, attn_mask=~mask.squeeze(2))\n",
    "#         output *= mask\n",
    "#         output = self.pos_ff(output)\n",
    "#         output *= mask\n",
    "#         return output\n",
    "\n",
    "\n",
    "# def mask_from_lens(lens, max_len: Optional[int] = None):\n",
    "#     if max_len is None:\n",
    "#         max_len = lens.max()\n",
    "#     ids = torch.arange(0, max_len, device=lens.device, dtype=lens.dtype)\n",
    "#     mask = torch.lt(ids, lens.unsqueeze(1))\n",
    "#     return mask\n",
    "\n",
    "\n",
    "# class FFTransformer(nn.Module):\n",
    "#     def __init__(self, n_layer, n_head, d_model, d_head, d_inner, kernel_size, dropout, dropatt, dropemb=0.0, pre_lnorm=False):\n",
    "#         super(FFTransformer, self).__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.n_head = n_head\n",
    "#         self.d_head = d_head\n",
    "\n",
    "#         self.drop = nn.Dropout(dropemb)\n",
    "#         self.layers = nn.ModuleList()\n",
    "\n",
    "#         for _ in range(n_layer):\n",
    "#             self.layers.append(\n",
    "#                 TransformerLayer(\n",
    "#                     n_head, d_model, d_head, d_inner, kernel_size, dropout,\n",
    "#                     dropatt=dropatt, pre_lnorm=pre_lnorm\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#     def forward(self, dec_inp, seq_lens=None, conditioning=0):\n",
    "#         mask = (dec_inp != 0).unsqueeze(2)\n",
    "#         out = self.drop(dec_inp + conditioning)\n",
    "\n",
    "#         for layer in self.layers:\n",
    "#             out = layer(out, mask=mask)\n",
    "\n",
    "#         out = self.drop(out)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseConvFF(nn.Module):\n",
    "    def __init__(self, d_model, d_inner, kernel_size, dropout, pre_lnorm=False):\n",
    "        super(PositionwiseConvFF, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.CoreNet = nn.Sequential(\n",
    "            nn.Conv1d(d_model, d_inner, kernel_size, 1, (kernel_size // 2)),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout),  # worse convergence\n",
    "            nn.Conv1d(d_inner, d_model, kernel_size, 1, (kernel_size // 2)),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.pre_lnorm = pre_lnorm\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return self._forward(inp)\n",
    "\n",
    "    def _forward(self, inp):\n",
    "        if self.pre_lnorm:\n",
    "            # layer normalization + positionwise feed-forward\n",
    "            core_out = inp.transpose(1, 2)\n",
    "            core_out = self.CoreNet(self.layer_norm(core_out).to(inp.dtype))\n",
    "            core_out = core_out.transpose(1, 2)\n",
    "\n",
    "            # residual connection\n",
    "            output = core_out + inp\n",
    "        else:\n",
    "            # positionwise feed-forward\n",
    "            core_out = inp.transpose(1, 2)\n",
    "            core_out = self.CoreNet(core_out)\n",
    "            core_out = core_out.transpose(1, 2)\n",
    "\n",
    "            # residual connection + layer normalization\n",
    "            output = self.layer_norm(inp + core_out).to(inp.dtype)\n",
    "\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttn(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0.1,\n",
    "                 pre_lnorm=False):\n",
    "        super(MultiHeadAttn, self).__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_head\n",
    "        self.scale = 1 / (d_head ** 0.5)\n",
    "        self.pre_lnorm = pre_lnorm\n",
    "\n",
    "        self.qkv_net = nn.Linear(d_model, 3 * n_head * d_head)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.dropatt = nn.Dropout(dropatt)\n",
    "        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, inp, attn_mask=None):\n",
    "        return self._forward(inp, attn_mask)\n",
    "\n",
    "    def _forward(self, inp, attn_mask=None):\n",
    "        residual = inp\n",
    "\n",
    "        if self.pre_lnorm:\n",
    "            # layer normalization\n",
    "            inp = self.layer_norm(inp)\n",
    "\n",
    "        n_head, d_head = self.n_head, self.d_head\n",
    "\n",
    "        head_q, head_k, head_v = torch.chunk(self.qkv_net(inp), 3, dim=2)\n",
    "        head_q = head_q.view(inp.size(0), inp.size(1), n_head, d_head)\n",
    "        head_k = head_k.view(inp.size(0), inp.size(1), n_head, d_head)\n",
    "        head_v = head_v.view(inp.size(0), inp.size(1), n_head, d_head)\n",
    "\n",
    "        q = head_q.permute(2, 0, 1, 3).reshape(-1, inp.size(1), d_head)\n",
    "        k = head_k.permute(2, 0, 1, 3).reshape(-1, inp.size(1), d_head)\n",
    "        v = head_v.permute(2, 0, 1, 3).reshape(-1, inp.size(1), d_head)\n",
    "\n",
    "        attn_score = torch.bmm(q, k.transpose(1, 2))\n",
    "        attn_score.mul_(self.scale)\n",
    "        print(attn_score.shape)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_score = attn_score.masked_fill(attn_mask == 0, float('-inf'))        \n",
    "\n",
    "        # if attn_mask is not None:\n",
    "        #     attn_mask = attn_mask.unsqueeze(1).to(attn_score.dtype)\n",
    "        #     attn_mask = attn_mask.repeat(n_head, attn_mask.size(2), 1)\n",
    "        #     print(attn_mask.shape)\n",
    "        #     attn_score.masked_fill_(attn_mask.to(torch.bool), -float('inf'))\n",
    "\n",
    "        attn_prob = F.softmax(attn_score, dim=2)\n",
    "        attn_prob = self.dropatt(attn_prob)\n",
    "        attn_vec = torch.bmm(attn_prob, v)\n",
    "\n",
    "        attn_vec = attn_vec.view(n_head, inp.size(0), inp.size(1), d_head)\n",
    "        attn_vec = attn_vec.permute(1, 2, 0, 3).contiguous().view(\n",
    "            inp.size(0), inp.size(1), n_head * d_head)\n",
    "\n",
    "        # linear projection\n",
    "        attn_out = self.o_net(attn_vec)\n",
    "        attn_out = self.drop(attn_out)\n",
    "\n",
    "        if self.pre_lnorm:\n",
    "            # residual connection\n",
    "            output = residual + attn_out\n",
    "        else:\n",
    "            # residual connection + layer normalization\n",
    "            output = self.layer_norm(residual + attn_out)\n",
    "\n",
    "        output = output.to(attn_out.dtype)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_head, d_inner, kernel_size, dropout,\n",
    "                 **kwargs):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        self.dec_attn = MultiHeadAttn(n_head, d_model, d_head, dropout, **kwargs)\n",
    "        self.pos_ff = PositionwiseConvFF(d_model, d_inner, kernel_size, dropout,\n",
    "                                         pre_lnorm=kwargs.get('pre_lnorm'))\n",
    "\n",
    "    def forward(self, dec_inp, mask=None):\n",
    "        \n",
    "        output = self.dec_attn(dec_inp, attn_mask=mask)\n",
    "        print(output.shape)\n",
    "        print(mask.shape)\n",
    "        # output = self.dec_attn(dec_inp)\n",
    "        # output *= mask\n",
    "        output = self.pos_ff(output)\n",
    "        # output *= mask\n",
    "        return output\n",
    "    \n",
    "class FFTransformer(nn.Module):\n",
    "    def __init__(self, n_layer, n_head, d_model, d_head, d_inner, kernel_size,\n",
    "                 dropout, dropatt, dropemb=0.0, embed_input=True,\n",
    "                 n_embed=None, d_embed=None, pre_lnorm=False):\n",
    "        super(FFTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_head\n",
    "        # self.padding_idx = padding_idx\n",
    "\n",
    "        # if embed_input:\n",
    "        #     self.word_emb = nn.Embedding(n_embed, d_embed or d_model,\n",
    "        #                                  padding_idx=self.padding_idx)\n",
    "        # else:\n",
    "        #     self.word_emb = None\n",
    "\n",
    "        # self.pos_emb = PositionalEmbedding(self.d_model)\n",
    "        self.drop = nn.Dropout(dropemb)\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for _ in range(n_layer):\n",
    "            self.layers.append(\n",
    "                TransformerLayer(\n",
    "                    n_head, d_model, d_head, d_inner, kernel_size, dropout,\n",
    "                    dropatt=dropatt, pre_lnorm=pre_lnorm)\n",
    "            )\n",
    "\n",
    "    def forward(self, dec_inp, mask=None):\n",
    "        # if self.word_emb is None:\n",
    "        #     inp = dec_inp\n",
    "        #     mask = mask_from_lens(seq_lens).unsqueeze(2)\n",
    "        # else:\n",
    "        #     inp = self.word_emb(dec_inp)\n",
    "        #     # [bsz x L x 1]\n",
    "        #     mask = (dec_inp != self.padding_idx).unsqueeze(2)\n",
    "\n",
    "        # pos_seq = torch.arange(inp.size(1), device=inp.device).to(inp.dtype)\n",
    "        # pos_emb = self.pos_emb(pos_seq) * mask\n",
    "\n",
    "        # out = self.drop(inp + pos_emb + conditioning)\n",
    "        # mask = mask_from_lens(seq_lens).unsqueeze(2)\n",
    "        # mask = (dec_inp != self.padding_idx).unsqueeze(2)\n",
    "        # print(mask.shape)\n",
    "        out = self.drop(dec_inp )\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, mask=mask)\n",
    "            # out = layer(out)\n",
    "\n",
    "        out = self.drop(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynthesizerModel(nn.Module):\n",
    "    def __init__(self, content_dim, speaker_dim, projected_dim):\n",
    "        super(SynthesizerModel, self).__init__()\n",
    "        \n",
    "        # Linear layer to project speaker embeddings to the same dimension as content embeddings\n",
    "        self.speaker_projection = nn.Linear(speaker_dim, projected_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self, content_embeddings, speaker_embeddings):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - content_embeddings: Tensor of shape [batch_size, content_dim, time_steps]\n",
    "        - speaker_embeddings: Tensor of shape [batch_size, speaker_dim]\n",
    "\n",
    "        Returns:\n",
    "        - Processed output\n",
    "        \"\"\"\n",
    "        # Project the speaker embeddings from [batch_size, speaker_dim] to [batch_size, projected_dim]\n",
    "        projected_speaker_embeddings = self.speaker_projection(speaker_embeddings)  # Shape: [batch_size, projected_dim]\n",
    "\n",
    "        # Expand the speaker embeddings across the time dimension to match the content embeddings\n",
    "        # Resulting shape: [batch_size, projected_dim, time_steps]\n",
    "        projected_speaker_embeddings = projected_speaker_embeddings.unsqueeze(2).expand(-1, -1, content_embeddings.size(2))\n",
    "        \n",
    "        # Concatenate the content and speaker embeddings along the feature dimension\n",
    "        combined_embeddings = torch.cat([content_embeddings, projected_speaker_embeddings], dim=1)\n",
    "\n",
    "        \n",
    "        return combined_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 512, 199])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "content_dim = 256\n",
    "speaker_dim = 192\n",
    "time_steps = 145\n",
    "projected_dim = 256  # Project speaker embeddings to this dimension\n",
    "\n",
    "# Create a dummy batch of inputs\n",
    "# content_embeddings = torch.randn(batch_size, content_dim, time_steps)\n",
    "# speaker_embeddings = torch.randn(batch_size, speaker_dim)\n",
    "\n",
    "# Initialize the model\n",
    "model = SynthesizerModel(content_dim=content_dim, speaker_dim=speaker_dim, projected_dim=projected_dim)\n",
    "\n",
    "# Forward pass\n",
    "output = model(content_embeddings, speaker_embeddings)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_encoder = FFTransformer(\n",
    "    n_layer=6, n_head=1,\n",
    "    d_model=512,\n",
    "    d_head=64,\n",
    "    d_inner=1536,\n",
    "    kernel_size=3,\n",
    "    dropout=0.1,\n",
    "    dropatt=0.1,\n",
    "    dropemb=0.0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 199, 199])\n",
      "torch.Size([4, 199, 512])\n",
      "torch.Size([4, 1, 199])\n",
      "torch.Size([4, 199, 199])\n",
      "torch.Size([4, 199, 512])\n",
      "torch.Size([4, 1, 199])\n",
      "torch.Size([4, 199, 199])\n",
      "torch.Size([4, 199, 512])\n",
      "torch.Size([4, 1, 199])\n",
      "torch.Size([4, 199, 199])\n",
      "torch.Size([4, 199, 512])\n",
      "torch.Size([4, 1, 199])\n",
      "torch.Size([4, 199, 199])\n",
      "torch.Size([4, 199, 512])\n",
      "torch.Size([4, 1, 199])\n",
      "torch.Size([4, 199, 199])\n",
      "torch.Size([4, 199, 512])\n",
      "torch.Size([4, 1, 199])\n"
     ]
    }
   ],
   "source": [
    "output_permute = output.permute(0,2,1)\n",
    "padding_mask_permute = padding_mask.permute(0,2,1)\n",
    "mask_reduced = padding_mask_permute[:, :, 0]\n",
    "mask_reduced = mask_reduced.unsqueeze(1)\n",
    "encoded_result = custom_encoder(output_permute,mask_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 199, 512])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 199, 256])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask_permute.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_reduced = padding_mask_permute[:, :, 0]\n",
    "mask_reduced = mask_reduced.unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 199])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 199, 512])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_permute.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim =512\n",
    "query_fc = nn.Linear(input_dim, input_dim)\n",
    "key_fc = nn.Linear(input_dim, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = query_fc(output_permute)\n",
    "keys = key_fc(output_permute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 199, 512])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 199, 512])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_length, _ = output_permute.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[4, 199, 1, 64]' is invalid for input of size 407552",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[43mqueries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (batch_size, num_heads, seq_length, head_dim)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m keys \u001b[38;5;241m=\u001b[39m keys\u001b[38;5;241m.\u001b[39mview(batch_size, seq_length, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[4, 199, 1, 64]' is invalid for input of size 407552"
     ]
    }
   ],
   "source": [
    "queries = queries.view(batch_size, seq_length, 1, 64).transpose(1, 2)  # (batch_size, num_heads, seq_length, head_dim)\n",
    "keys = keys.view(batch_size, seq_length, 1, 64).transpose(1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4,1,199,64  4,1,64,199"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
